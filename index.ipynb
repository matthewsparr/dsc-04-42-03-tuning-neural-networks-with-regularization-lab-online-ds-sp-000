{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Your code here; import some packages/modules you plan to use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; load and preview the dataset\n",
    "data = pd.read_csv('Bank_complaints.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "random.seed(123)\n",
    "sample = data.sample(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "complaints = sample['Consumer complaint narrative']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "products = sample.Product\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.fit(products)\n",
    "products_num = encoder.transform(products) \n",
    "\n",
    "products_en = to_categorical(products_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, products_en, test_size=.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.9503 - acc: 0.1581 - val_loss: 1.9339 - val_acc: 0.1760\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.9251 - acc: 0.1949 - val_loss: 1.9129 - val_acc: 0.2050\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.9080 - acc: 0.2155 - val_loss: 1.8949 - val_acc: 0.2380\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8902 - acc: 0.2436 - val_loss: 1.8749 - val_acc: 0.2620\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.8699 - acc: 0.2668 - val_loss: 1.8527 - val_acc: 0.2890\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8463 - acc: 0.2947 - val_loss: 1.8278 - val_acc: 0.3150\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8196 - acc: 0.3207 - val_loss: 1.8000 - val_acc: 0.3450\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.7895 - acc: 0.3441 - val_loss: 1.7684 - val_acc: 0.3790\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.7558 - acc: 0.3737 - val_loss: 1.7327 - val_acc: 0.3990\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.7182 - acc: 0.3956 - val_loss: 1.6943 - val_acc: 0.4120\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.6773 - acc: 0.4165 - val_loss: 1.6522 - val_acc: 0.4410\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.6324 - acc: 0.4391 - val_loss: 1.6061 - val_acc: 0.4480\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5846 - acc: 0.4589 - val_loss: 1.5582 - val_acc: 0.4760\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.5341 - acc: 0.4824 - val_loss: 1.5100 - val_acc: 0.5080\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4827 - acc: 0.5133 - val_loss: 1.4587 - val_acc: 0.5240\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4304 - acc: 0.5353 - val_loss: 1.4092 - val_acc: 0.5560\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.3781 - acc: 0.5559 - val_loss: 1.3589 - val_acc: 0.5810\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3266 - acc: 0.5787 - val_loss: 1.3113 - val_acc: 0.5950\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.2752 - acc: 0.5973 - val_loss: 1.2627 - val_acc: 0.6150\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.2247 - acc: 0.6188 - val_loss: 1.2163 - val_acc: 0.6180\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1762 - acc: 0.6348 - val_loss: 1.1734 - val_acc: 0.6310\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.1315 - acc: 0.6467 - val_loss: 1.1342 - val_acc: 0.6360\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0898 - acc: 0.6593 - val_loss: 1.0961 - val_acc: 0.6520\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0515 - acc: 0.6707 - val_loss: 1.0605 - val_acc: 0.6600\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0157 - acc: 0.6813 - val_loss: 1.0291 - val_acc: 0.6630\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9828 - acc: 0.6924 - val_loss: 0.9989 - val_acc: 0.6770\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9527 - acc: 0.7029 - val_loss: 0.9736 - val_acc: 0.6880\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9249 - acc: 0.7085 - val_loss: 0.9498 - val_acc: 0.6920\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8990 - acc: 0.7152 - val_loss: 0.9323 - val_acc: 0.6840\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8758 - acc: 0.7215 - val_loss: 0.9080 - val_acc: 0.6970\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8532 - acc: 0.7272 - val_loss: 0.8895 - val_acc: 0.6980\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8329 - acc: 0.7316 - val_loss: 0.8749 - val_acc: 0.7000\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8141 - acc: 0.7372 - val_loss: 0.8599 - val_acc: 0.7070\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7962 - acc: 0.7385 - val_loss: 0.8429 - val_acc: 0.7140\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7794 - acc: 0.7432 - val_loss: 0.8288 - val_acc: 0.7180\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7639 - acc: 0.7459 - val_loss: 0.8174 - val_acc: 0.7210\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7496 - acc: 0.7501 - val_loss: 0.8061 - val_acc: 0.7210\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7362 - acc: 0.7539 - val_loss: 0.7947 - val_acc: 0.7250\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7234 - acc: 0.7552 - val_loss: 0.7855 - val_acc: 0.7280\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7120 - acc: 0.7600 - val_loss: 0.7773 - val_acc: 0.7320\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7002 - acc: 0.7611 - val_loss: 0.7687 - val_acc: 0.7330\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6892 - acc: 0.7641 - val_loss: 0.7603 - val_acc: 0.7290\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6790 - acc: 0.7675 - val_loss: 0.7528 - val_acc: 0.7310\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6690 - acc: 0.7711 - val_loss: 0.7480 - val_acc: 0.7350\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6597 - acc: 0.7740 - val_loss: 0.7400 - val_acc: 0.7340\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6511 - acc: 0.7761 - val_loss: 0.7349 - val_acc: 0.7360\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6421 - acc: 0.7788 - val_loss: 0.7265 - val_acc: 0.7410\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.6341 - acc: 0.7795 - val_loss: 0.7215 - val_acc: 0.7430\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.6259 - acc: 0.7827 - val_loss: 0.7189 - val_acc: 0.7410\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.6182 - acc: 0.7869 - val_loss: 0.7131 - val_acc: 0.7430\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.6111 - acc: 0.7872 - val_loss: 0.7088 - val_acc: 0.7470\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.6045 - acc: 0.7889 - val_loss: 0.7049 - val_acc: 0.7470\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5975 - acc: 0.7913 - val_loss: 0.7012 - val_acc: 0.7470\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.5910 - acc: 0.7913 - val_loss: 0.6969 - val_acc: 0.7450\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.5848 - acc: 0.7947 - val_loss: 0.6934 - val_acc: 0.7550\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.5783 - acc: 0.7997 - val_loss: 0.6902 - val_acc: 0.7520\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.5725 - acc: 0.7991 - val_loss: 0.6880 - val_acc: 0.7480\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.5658 - acc: 0.8041 - val_loss: 0.6842 - val_acc: 0.7520\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.5611 - acc: 0.8033 - val_loss: 0.6817 - val_acc: 0.7480\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.5558 - acc: 0.8067 - val_loss: 0.6786 - val_acc: 0.7490\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5498 - acc: 0.8095 - val_loss: 0.6761 - val_acc: 0.7520\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.5445 - acc: 0.8088 - val_loss: 0.6743 - val_acc: 0.7560\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5395 - acc: 0.8116 - val_loss: 0.6753 - val_acc: 0.7610\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5349 - acc: 0.8137 - val_loss: 0.6708 - val_acc: 0.7520\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.5297 - acc: 0.8169 - val_loss: 0.6682 - val_acc: 0.7510\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.5249 - acc: 0.8176 - val_loss: 0.6696 - val_acc: 0.7540\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.5209 - acc: 0.8204 - val_loss: 0.6644 - val_acc: 0.7530\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.5157 - acc: 0.8219 - val_loss: 0.6661 - val_acc: 0.7500\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.5116 - acc: 0.8215 - val_loss: 0.6626 - val_acc: 0.7520\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.5070 - acc: 0.8216 - val_loss: 0.6641 - val_acc: 0.7520\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.5024 - acc: 0.8244 - val_loss: 0.6569 - val_acc: 0.7620\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4983 - acc: 0.8279 - val_loss: 0.6593 - val_acc: 0.7620\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4946 - acc: 0.8291 - val_loss: 0.6563 - val_acc: 0.7580\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4900 - acc: 0.8309 - val_loss: 0.6555 - val_acc: 0.7580\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.4861 - acc: 0.8323 - val_loss: 0.6547 - val_acc: 0.7640\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4819 - acc: 0.8341 - val_loss: 0.6521 - val_acc: 0.7650\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.4782 - acc: 0.8332 - val_loss: 0.6516 - val_acc: 0.7680\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4747 - acc: 0.8345 - val_loss: 0.6515 - val_acc: 0.7650\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.4704 - acc: 0.8355 - val_loss: 0.6494 - val_acc: 0.7620\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.4668 - acc: 0.8371 - val_loss: 0.6521 - val_acc: 0.7610\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.4632 - acc: 0.8393 - val_loss: 0.6478 - val_acc: 0.7660\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4599 - acc: 0.8407 - val_loss: 0.6472 - val_acc: 0.7640\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.4562 - acc: 0.8445 - val_loss: 0.6471 - val_acc: 0.7610\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4524 - acc: 0.8435 - val_loss: 0.6493 - val_acc: 0.7560\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.4494 - acc: 0.8459 - val_loss: 0.6485 - val_acc: 0.7660\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4456 - acc: 0.8483 - val_loss: 0.6465 - val_acc: 0.7650\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4424 - acc: 0.8484 - val_loss: 0.6454 - val_acc: 0.7670\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4398 - acc: 0.8467 - val_loss: 0.6462 - val_acc: 0.7690\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4359 - acc: 0.8507 - val_loss: 0.6432 - val_acc: 0.7680\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4326 - acc: 0.8536 - val_loss: 0.6433 - val_acc: 0.7670\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4297 - acc: 0.8521 - val_loss: 0.6433 - val_acc: 0.7680\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.4269 - acc: 0.8532 - val_loss: 0.6440 - val_acc: 0.7650\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4233 - acc: 0.8545 - val_loss: 0.6425 - val_acc: 0.7680\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.4198 - acc: 0.8557 - val_loss: 0.6423 - val_acc: 0.7680\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4171 - acc: 0.8575 - val_loss: 0.6418 - val_acc: 0.7720\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4140 - acc: 0.8592 - val_loss: 0.6453 - val_acc: 0.7680\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.4114 - acc: 0.8607 - val_loss: 0.6430 - val_acc: 0.7660\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4086 - acc: 0.8612 - val_loss: 0.6465 - val_acc: 0.7630\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4055 - acc: 0.8629 - val_loss: 0.6431 - val_acc: 0.7730\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4025 - acc: 0.8653 - val_loss: 0.6415 - val_acc: 0.7710\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3996 - acc: 0.8647 - val_loss: 0.6411 - val_acc: 0.7730\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.3969 - acc: 0.8655 - val_loss: 0.6426 - val_acc: 0.7690\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3939 - acc: 0.8661 - val_loss: 0.6439 - val_acc: 0.7730\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3914 - acc: 0.8695 - val_loss: 0.6447 - val_acc: 0.7670\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3887 - acc: 0.8695 - val_loss: 0.6432 - val_acc: 0.7740\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.3864 - acc: 0.8701 - val_loss: 0.6436 - val_acc: 0.7710\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3833 - acc: 0.8713 - val_loss: 0.6440 - val_acc: 0.7670\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3802 - acc: 0.8735 - val_loss: 0.6429 - val_acc: 0.7700\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.3782 - acc: 0.8728 - val_loss: 0.6446 - val_acc: 0.7680\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.3754 - acc: 0.8761 - val_loss: 0.6447 - val_acc: 0.7780\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.3731 - acc: 0.8771 - val_loss: 0.6431 - val_acc: 0.7760\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.3704 - acc: 0.8765 - val_loss: 0.6461 - val_acc: 0.7720\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.3678 - acc: 0.8769 - val_loss: 0.6438 - val_acc: 0.7670\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.3653 - acc: 0.8775 - val_loss: 0.6458 - val_acc: 0.7730\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.3628 - acc: 0.8793 - val_loss: 0.6453 - val_acc: 0.7670\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3611 - acc: 0.8808 - val_loss: 0.6481 - val_acc: 0.7630\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.3580 - acc: 0.8829 - val_loss: 0.6475 - val_acc: 0.7760\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3559 - acc: 0.8819 - val_loss: 0.6501 - val_acc: 0.7700\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3533 - acc: 0.8857 - val_loss: 0.6447 - val_acc: 0.7720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.3512 - acc: 0.8840 - val_loss: 0.6474 - val_acc: 0.7720\n"
     ]
    }
   ],
   "source": [
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 101us/step - loss: 0.3487 - acc: 0.8852 - val_loss: 0.6514 - val_acc: 0.7730\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3467 - acc: 0.8869 - val_loss: 0.6461 - val_acc: 0.7710\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3441 - acc: 0.8896 - val_loss: 0.6452 - val_acc: 0.7740\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.3416 - acc: 0.8884 - val_loss: 0.6485 - val_acc: 0.7660\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.3397 - acc: 0.8893 - val_loss: 0.6473 - val_acc: 0.7730\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.3373 - acc: 0.8896 - val_loss: 0.6483 - val_acc: 0.7710\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.3352 - acc: 0.8917 - val_loss: 0.6483 - val_acc: 0.7730\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.3333 - acc: 0.8931 - val_loss: 0.6487 - val_acc: 0.7720\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.3311 - acc: 0.8931 - val_loss: 0.6491 - val_acc: 0.7740\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.3286 - acc: 0.8931 - val_loss: 0.6521 - val_acc: 0.7710\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.3267 - acc: 0.8953 - val_loss: 0.6512 - val_acc: 0.7740\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.3242 - acc: 0.8961 - val_loss: 0.6504 - val_acc: 0.7710\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.3227 - acc: 0.8968 - val_loss: 0.6526 - val_acc: 0.7720\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.3206 - acc: 0.8972 - val_loss: 0.6537 - val_acc: 0.7750\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.3183 - acc: 0.8988 - val_loss: 0.6536 - val_acc: 0.7700\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.3162 - acc: 0.8995 - val_loss: 0.6559 - val_acc: 0.7710\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.3142 - acc: 0.9003 - val_loss: 0.6545 - val_acc: 0.7720\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.3123 - acc: 0.9011 - val_loss: 0.6537 - val_acc: 0.7740\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.3102 - acc: 0.9015 - val_loss: 0.6562 - val_acc: 0.7720\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.3082 - acc: 0.9031 - val_loss: 0.6576 - val_acc: 0.7730\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.3066 - acc: 0.9029 - val_loss: 0.6579 - val_acc: 0.7760\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.3045 - acc: 0.9047 - val_loss: 0.6584 - val_acc: 0.7700\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.3027 - acc: 0.9049 - val_loss: 0.6634 - val_acc: 0.7740\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.3005 - acc: 0.9067 - val_loss: 0.6608 - val_acc: 0.7740\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.2986 - acc: 0.9076 - val_loss: 0.6595 - val_acc: 0.7740\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.2971 - acc: 0.9071 - val_loss: 0.6627 - val_acc: 0.7760\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.2948 - acc: 0.9089 - val_loss: 0.6614 - val_acc: 0.7750\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.2928 - acc: 0.9108 - val_loss: 0.6623 - val_acc: 0.7770\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.2912 - acc: 0.9095 - val_loss: 0.6663 - val_acc: 0.7750\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.2892 - acc: 0.9120 - val_loss: 0.6736 - val_acc: 0.7700\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.2876 - acc: 0.9123 - val_loss: 0.6661 - val_acc: 0.7750\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.2856 - acc: 0.9127 - val_loss: 0.6645 - val_acc: 0.7750\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.2841 - acc: 0.9149 - val_loss: 0.6664 - val_acc: 0.7720\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.2821 - acc: 0.9131 - val_loss: 0.6675 - val_acc: 0.7760\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.2803 - acc: 0.9151 - val_loss: 0.6686 - val_acc: 0.7780\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.2786 - acc: 0.9156 - val_loss: 0.6742 - val_acc: 0.7710\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.2768 - acc: 0.9165 - val_loss: 0.6685 - val_acc: 0.7760\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.2748 - acc: 0.9169 - val_loss: 0.6740 - val_acc: 0.7850\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.2737 - acc: 0.9163 - val_loss: 0.6713 - val_acc: 0.7730\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.2714 - acc: 0.9201 - val_loss: 0.6733 - val_acc: 0.7770\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.2697 - acc: 0.9200 - val_loss: 0.6746 - val_acc: 0.7720\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.2682 - acc: 0.9200 - val_loss: 0.6731 - val_acc: 0.7760\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.2662 - acc: 0.9193 - val_loss: 0.6797 - val_acc: 0.7680\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.2647 - acc: 0.9208 - val_loss: 0.6761 - val_acc: 0.7740\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.2629 - acc: 0.9207 - val_loss: 0.6793 - val_acc: 0.7790\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.2616 - acc: 0.9229 - val_loss: 0.6801 - val_acc: 0.7710\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.2596 - acc: 0.9219 - val_loss: 0.6819 - val_acc: 0.7770\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.2583 - acc: 0.9232 - val_loss: 0.6784 - val_acc: 0.7790\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.2566 - acc: 0.9245 - val_loss: 0.6811 - val_acc: 0.7740\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.2552 - acc: 0.9267 - val_loss: 0.6792 - val_acc: 0.7770\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.2535 - acc: 0.9260 - val_loss: 0.6804 - val_acc: 0.7780\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.2517 - acc: 0.9268 - val_loss: 0.6835 - val_acc: 0.7820\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.2503 - acc: 0.9276 - val_loss: 0.6848 - val_acc: 0.7770\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.2489 - acc: 0.9280 - val_loss: 0.6872 - val_acc: 0.7760\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.2472 - acc: 0.9284 - val_loss: 0.6875 - val_acc: 0.7730\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.2460 - acc: 0.9296 - val_loss: 0.6862 - val_acc: 0.7780\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.2440 - acc: 0.9304 - val_loss: 0.6850 - val_acc: 0.7720\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.2431 - acc: 0.9296 - val_loss: 0.6862 - val_acc: 0.7770\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.2412 - acc: 0.9315 - val_loss: 0.6874 - val_acc: 0.7760\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.2397 - acc: 0.9308 - val_loss: 0.6895 - val_acc: 0.7710\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.2382 - acc: 0.9316 - val_loss: 0.6935 - val_acc: 0.7740\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.2365 - acc: 0.9333 - val_loss: 0.6890 - val_acc: 0.7800\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.2353 - acc: 0.9336 - val_loss: 0.6915 - val_acc: 0.7740\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.2336 - acc: 0.9347 - val_loss: 0.6916 - val_acc: 0.7790\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.2325 - acc: 0.9353 - val_loss: 0.6960 - val_acc: 0.7750\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.2311 - acc: 0.9352 - val_loss: 0.6947 - val_acc: 0.7720\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.2299 - acc: 0.9365 - val_loss: 0.6962 - val_acc: 0.7790\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.2278 - acc: 0.9364 - val_loss: 0.6961 - val_acc: 0.7800\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.2270 - acc: 0.9357 - val_loss: 0.6982 - val_acc: 0.7730\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.2252 - acc: 0.9372 - val_loss: 0.7004 - val_acc: 0.7740\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.2238 - acc: 0.9377 - val_loss: 0.7006 - val_acc: 0.7770\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.2224 - acc: 0.9383 - val_loss: 0.7015 - val_acc: 0.7770\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.2210 - acc: 0.9399 - val_loss: 0.7016 - val_acc: 0.7780\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.2196 - acc: 0.9403 - val_loss: 0.7040 - val_acc: 0.7750\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.2182 - acc: 0.9399 - val_loss: 0.7086 - val_acc: 0.7690\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.2172 - acc: 0.9417 - val_loss: 0.7051 - val_acc: 0.7720\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.2159 - acc: 0.9413 - val_loss: 0.7054 - val_acc: 0.7730\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.2146 - acc: 0.9421 - val_loss: 0.7071 - val_acc: 0.7730\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.2134 - acc: 0.9428 - val_loss: 0.7158 - val_acc: 0.7740\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.2121 - acc: 0.9439 - val_loss: 0.7119 - val_acc: 0.7720\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.2109 - acc: 0.9431 - val_loss: 0.7133 - val_acc: 0.7730\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.2095 - acc: 0.9432 - val_loss: 0.7109 - val_acc: 0.7720\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.2081 - acc: 0.9455 - val_loss: 0.7119 - val_acc: 0.7770\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.2070 - acc: 0.9451 - val_loss: 0.7138 - val_acc: 0.7750\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.2058 - acc: 0.9456 - val_loss: 0.7149 - val_acc: 0.7750\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.2045 - acc: 0.9457 - val_loss: 0.7200 - val_acc: 0.7690\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.2033 - acc: 0.9464 - val_loss: 0.7151 - val_acc: 0.7750\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.2018 - acc: 0.9473 - val_loss: 0.7224 - val_acc: 0.7710\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.2007 - acc: 0.9484 - val_loss: 0.7177 - val_acc: 0.7750\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.1992 - acc: 0.9496 - val_loss: 0.7198 - val_acc: 0.7730\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.1983 - acc: 0.9477 - val_loss: 0.7245 - val_acc: 0.7710\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.1970 - acc: 0.9488 - val_loss: 0.7271 - val_acc: 0.7710\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.1960 - acc: 0.9485 - val_loss: 0.7241 - val_acc: 0.7750\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.1946 - acc: 0.9503 - val_loss: 0.7264 - val_acc: 0.7740\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.1936 - acc: 0.9493 - val_loss: 0.7300 - val_acc: 0.7680\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.1925 - acc: 0.9512 - val_loss: 0.7298 - val_acc: 0.7690\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.1912 - acc: 0.9512 - val_loss: 0.7282 - val_acc: 0.7730\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.1901 - acc: 0.9508 - val_loss: 0.7325 - val_acc: 0.7670\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.1887 - acc: 0.9511 - val_loss: 0.7338 - val_acc: 0.7660\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.1878 - acc: 0.9513 - val_loss: 0.7347 - val_acc: 0.7690\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.1868 - acc: 0.9521 - val_loss: 0.7322 - val_acc: 0.7730\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.1856 - acc: 0.9524 - val_loss: 0.7361 - val_acc: 0.7690\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.1845 - acc: 0.9529 - val_loss: 0.7374 - val_acc: 0.7670\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.1834 - acc: 0.9527 - val_loss: 0.7413 - val_acc: 0.7630\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.1820 - acc: 0.9541 - val_loss: 0.7381 - val_acc: 0.7700\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.1813 - acc: 0.9543 - val_loss: 0.7396 - val_acc: 0.7670\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.1802 - acc: 0.9533 - val_loss: 0.7411 - val_acc: 0.7740\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.1788 - acc: 0.9548 - val_loss: 0.7428 - val_acc: 0.7700\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.1780 - acc: 0.9557 - val_loss: 0.7457 - val_acc: 0.7650\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.1769 - acc: 0.9560 - val_loss: 0.7449 - val_acc: 0.7710\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.1759 - acc: 0.9552 - val_loss: 0.7469 - val_acc: 0.7660\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.1747 - acc: 0.9561 - val_loss: 0.7483 - val_acc: 0.7720\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.1738 - acc: 0.9567 - val_loss: 0.7523 - val_acc: 0.7700\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.1728 - acc: 0.9576 - val_loss: 0.7508 - val_acc: 0.7680\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.1717 - acc: 0.9569 - val_loss: 0.7521 - val_acc: 0.7600\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.1705 - acc: 0.9585 - val_loss: 0.7535 - val_acc: 0.7720\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.1700 - acc: 0.9571 - val_loss: 0.7525 - val_acc: 0.7720\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.1687 - acc: 0.9581 - val_loss: 0.7547 - val_acc: 0.7700\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.1674 - acc: 0.9585 - val_loss: 0.7564 - val_acc: 0.7660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.1668 - acc: 0.9581 - val_loss: 0.7578 - val_acc: 0.7630\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 35us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 28us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1643704978863398, 0.96]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7511380759874979, 0.7513333331743877]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.9480 - acc: 0.1541 - val_loss: 1.9339 - val_acc: 0.1650\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.9330 - acc: 0.173 - 0s 32us/step - loss: 1.9297 - acc: 0.1827 - val_loss: 1.9191 - val_acc: 0.2000\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.9131 - acc: 0.2093 - val_loss: 1.9028 - val_acc: 0.2270\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8948 - acc: 0.2328 - val_loss: 1.8838 - val_acc: 0.2540\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8738 - acc: 0.2591 - val_loss: 1.8606 - val_acc: 0.2800\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8487 - acc: 0.2817 - val_loss: 1.8327 - val_acc: 0.3030\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8188 - acc: 0.3117 - val_loss: 1.8006 - val_acc: 0.3310\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7842 - acc: 0.3408 - val_loss: 1.7644 - val_acc: 0.3580\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7453 - acc: 0.3685 - val_loss: 1.7241 - val_acc: 0.3920\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7026 - acc: 0.4000 - val_loss: 1.6816 - val_acc: 0.4200\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6570 - acc: 0.4277 - val_loss: 1.6347 - val_acc: 0.4520\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.6086 - acc: 0.4608 - val_loss: 1.5865 - val_acc: 0.4750\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.5583 - acc: 0.4863 - val_loss: 1.5355 - val_acc: 0.5100\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5064 - acc: 0.5059 - val_loss: 1.4843 - val_acc: 0.5280\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4534 - acc: 0.5299 - val_loss: 1.4331 - val_acc: 0.5420\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4012 - acc: 0.5453 - val_loss: 1.3835 - val_acc: 0.5570\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3503 - acc: 0.5644 - val_loss: 1.3362 - val_acc: 0.5730\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3015 - acc: 0.5793 - val_loss: 1.2928 - val_acc: 0.6010\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.2554 - acc: 0.5972 - val_loss: 1.2480 - val_acc: 0.6060\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.2118 - acc: 0.6089 - val_loss: 1.2099 - val_acc: 0.6120\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.1707 - acc: 0.6213 - val_loss: 1.1720 - val_acc: 0.6200\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1320 - acc: 0.6367 - val_loss: 1.1391 - val_acc: 0.6180\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0963 - acc: 0.6447 - val_loss: 1.1047 - val_acc: 0.6300\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0629 - acc: 0.6549 - val_loss: 1.0760 - val_acc: 0.6330\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0317 - acc: 0.6619 - val_loss: 1.0498 - val_acc: 0.6380\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0029 - acc: 0.6700 - val_loss: 1.0251 - val_acc: 0.6460\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9762 - acc: 0.6807 - val_loss: 1.0017 - val_acc: 0.6460\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9516 - acc: 0.6848 - val_loss: 0.9789 - val_acc: 0.6590\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9285 - acc: 0.6915 - val_loss: 0.9595 - val_acc: 0.6640\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9070 - acc: 0.6964 - val_loss: 0.9454 - val_acc: 0.6650\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8875 - acc: 0.7021 - val_loss: 0.9265 - val_acc: 0.6740\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8685 - acc: 0.7061 - val_loss: 0.9106 - val_acc: 0.6880\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8512 - acc: 0.7119 - val_loss: 0.8951 - val_acc: 0.6850\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.8339 - acc: 0.718 - 0s 39us/step - loss: 0.8347 - acc: 0.7175 - val_loss: 0.8813 - val_acc: 0.6970\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8194 - acc: 0.7199 - val_loss: 0.8689 - val_acc: 0.7020\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8056 - acc: 0.7223 - val_loss: 0.8570 - val_acc: 0.7030\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7917 - acc: 0.7289 - val_loss: 0.8483 - val_acc: 0.7100\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7788 - acc: 0.7305 - val_loss: 0.8387 - val_acc: 0.7090\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.7668 - acc: 0.7343 - val_loss: 0.8282 - val_acc: 0.7120\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7543 - acc: 0.7397 - val_loss: 0.8190 - val_acc: 0.7200\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.7437 - acc: 0.7431 - val_loss: 0.8113 - val_acc: 0.7200\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7335 - acc: 0.7443 - val_loss: 0.8029 - val_acc: 0.7230\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7231 - acc: 0.7499 - val_loss: 0.7974 - val_acc: 0.7230\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7133 - acc: 0.7539 - val_loss: 0.7902 - val_acc: 0.7270\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.7038 - acc: 0.7564 - val_loss: 0.7847 - val_acc: 0.7290\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.6949 - acc: 0.7600 - val_loss: 0.7786 - val_acc: 0.7270\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.6863 - acc: 0.7628 - val_loss: 0.7723 - val_acc: 0.7280\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6775 - acc: 0.7647 - val_loss: 0.7664 - val_acc: 0.7310\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.6689 - acc: 0.7685 - val_loss: 0.7623 - val_acc: 0.7320\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.6618 - acc: 0.7697 - val_loss: 0.7572 - val_acc: 0.7380\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.6541 - acc: 0.7744 - val_loss: 0.7509 - val_acc: 0.7400\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.6459 - acc: 0.7760 - val_loss: 0.7478 - val_acc: 0.7390\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.6389 - acc: 0.7780 - val_loss: 0.7430 - val_acc: 0.7430\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.6320 - acc: 0.7835 - val_loss: 0.7382 - val_acc: 0.7460\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.6248 - acc: 0.7837 - val_loss: 0.7344 - val_acc: 0.7440\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.6183 - acc: 0.7872 - val_loss: 0.7305 - val_acc: 0.7450\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.6116 - acc: 0.7857 - val_loss: 0.7268 - val_acc: 0.7480\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.6055 - acc: 0.7885 - val_loss: 0.7238 - val_acc: 0.7470\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.5999 - acc: 0.7913 - val_loss: 0.7243 - val_acc: 0.7490\n",
      "Epoch 60/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.5932 - acc: 0.7937 - val_loss: 0.7173 - val_acc: 0.7560\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 27us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 37us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5875496999104818, 0.7982666666666667]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7139892110824585, 0.7340000003178915]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 2.6056 - acc: 0.1520 - val_loss: 2.5861 - val_acc: 0.1550\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.5795 - acc: 0.1597 - val_loss: 2.5638 - val_acc: 0.1570\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.5585 - acc: 0.1759 - val_loss: 2.5427 - val_acc: 0.1740\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.5377 - acc: 0.1991 - val_loss: 2.5208 - val_acc: 0.2070\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 2.5157 - acc: 0.2247 - val_loss: 2.4967 - val_acc: 0.2510\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 2.4912 - acc: 0.2540 - val_loss: 2.4709 - val_acc: 0.2810\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 2.4640 - acc: 0.2881 - val_loss: 2.4422 - val_acc: 0.3150\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 2.4344 - acc: 0.3165 - val_loss: 2.4125 - val_acc: 0.3360\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 2.4028 - acc: 0.3467 - val_loss: 2.3800 - val_acc: 0.3660\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 2.3689 - acc: 0.3799 - val_loss: 2.3461 - val_acc: 0.3840\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 2.3329 - acc: 0.4009 - val_loss: 2.3094 - val_acc: 0.4240\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 2.2944 - acc: 0.4399 - val_loss: 2.2720 - val_acc: 0.4500\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 2.2534 - acc: 0.4628 - val_loss: 2.2306 - val_acc: 0.4830\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.2101 - acc: 0.4939 - val_loss: 2.1884 - val_acc: 0.5170\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.1644 - acc: 0.5247 - val_loss: 2.1432 - val_acc: 0.5380\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.1164 - acc: 0.5509 - val_loss: 2.0974 - val_acc: 0.5650\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 2.0672 - acc: 0.5761 - val_loss: 2.0496 - val_acc: 0.5830\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.0167 - acc: 0.5936 - val_loss: 2.0006 - val_acc: 0.6040\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.9653 - acc: 0.6119 - val_loss: 1.9523 - val_acc: 0.6170\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.9141 - acc: 0.6247 - val_loss: 1.9050 - val_acc: 0.6280\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.8638 - acc: 0.6371 - val_loss: 1.8582 - val_acc: 0.6380\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8150 - acc: 0.6497 - val_loss: 1.8119 - val_acc: 0.6410\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.7685 - acc: 0.6600 - val_loss: 1.7697 - val_acc: 0.6550\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.7241 - acc: 0.6713 - val_loss: 1.7282 - val_acc: 0.6710\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.6827 - acc: 0.6792 - val_loss: 1.6919 - val_acc: 0.6770\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.6443 - acc: 0.6887 - val_loss: 1.6543 - val_acc: 0.6750\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.6086 - acc: 0.6951 - val_loss: 1.6231 - val_acc: 0.6910\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.5754 - acc: 0.7027 - val_loss: 1.5945 - val_acc: 0.6910\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.5452 - acc: 0.7061 - val_loss: 1.5670 - val_acc: 0.6930\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.5169 - acc: 0.7119 - val_loss: 1.5427 - val_acc: 0.6970\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.4914 - acc: 0.7179 - val_loss: 1.5198 - val_acc: 0.7030\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.4673 - acc: 0.7215 - val_loss: 1.4983 - val_acc: 0.7050\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.4454 - acc: 0.7265 - val_loss: 1.4791 - val_acc: 0.7100\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.4245 - acc: 0.7293 - val_loss: 1.4628 - val_acc: 0.7050\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.4049 - acc: 0.7356 - val_loss: 1.4447 - val_acc: 0.7140\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3876 - acc: 0.7400 - val_loss: 1.4296 - val_acc: 0.7200\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.3704 - acc: 0.7448 - val_loss: 1.4193 - val_acc: 0.7130\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.3547 - acc: 0.7481 - val_loss: 1.4019 - val_acc: 0.7200\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.3395 - acc: 0.7528 - val_loss: 1.3895 - val_acc: 0.7250\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.3252 - acc: 0.7557 - val_loss: 1.3785 - val_acc: 0.7260\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.3117 - acc: 0.7567 - val_loss: 1.3681 - val_acc: 0.7290\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.2988 - acc: 0.7608 - val_loss: 1.3574 - val_acc: 0.7280\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.2865 - acc: 0.7656 - val_loss: 1.3468 - val_acc: 0.7270\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.2776 - acc: 0.764 - 0s 31us/step - loss: 1.2749 - acc: 0.7659 - val_loss: 1.3380 - val_acc: 0.7320\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.2634 - acc: 0.7696 - val_loss: 1.3308 - val_acc: 0.7350\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.2530 - acc: 0.7736 - val_loss: 1.3195 - val_acc: 0.7380\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.2423 - acc: 0.7729 - val_loss: 1.3165 - val_acc: 0.7260\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.2328 - acc: 0.7808 - val_loss: 1.3059 - val_acc: 0.7370\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.2229 - acc: 0.7805 - val_loss: 1.2980 - val_acc: 0.7360\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.2137 - acc: 0.7840 - val_loss: 1.2934 - val_acc: 0.7360\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.2043 - acc: 0.7899 - val_loss: 1.2847 - val_acc: 0.7420\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.1954 - acc: 0.7895 - val_loss: 1.2799 - val_acc: 0.7380\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1876 - acc: 0.7925 - val_loss: 1.2709 - val_acc: 0.7430\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1790 - acc: 0.7939 - val_loss: 1.2640 - val_acc: 0.7420\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1708 - acc: 0.7952 - val_loss: 1.2606 - val_acc: 0.7450\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.1627 - acc: 0.7980 - val_loss: 1.2570 - val_acc: 0.7430\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.1549 - acc: 0.7992 - val_loss: 1.2497 - val_acc: 0.7480\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1478 - acc: 0.7985 - val_loss: 1.2437 - val_acc: 0.7460\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.1402 - acc: 0.8027 - val_loss: 1.2382 - val_acc: 0.7450\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.1329 - acc: 0.8017 - val_loss: 1.2339 - val_acc: 0.7500\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.1262 - acc: 0.8044 - val_loss: 1.2299 - val_acc: 0.7480\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1190 - acc: 0.8075 - val_loss: 1.2237 - val_acc: 0.7430\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1122 - acc: 0.8076 - val_loss: 1.2209 - val_acc: 0.7450\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1056 - acc: 0.8096 - val_loss: 1.2145 - val_acc: 0.7500\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0993 - acc: 0.8100 - val_loss: 1.2125 - val_acc: 0.7460\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0925 - acc: 0.8163 - val_loss: 1.2064 - val_acc: 0.7470\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0867 - acc: 0.8137 - val_loss: 1.2023 - val_acc: 0.7550\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0805 - acc: 0.8164 - val_loss: 1.2001 - val_acc: 0.7520\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0743 - acc: 0.8176 - val_loss: 1.1950 - val_acc: 0.7530\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0680 - acc: 0.8204 - val_loss: 1.1924 - val_acc: 0.7520\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0618 - acc: 0.8213 - val_loss: 1.1882 - val_acc: 0.7510\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0567 - acc: 0.8229 - val_loss: 1.1838 - val_acc: 0.7570\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0504 - acc: 0.8257 - val_loss: 1.1790 - val_acc: 0.7600\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0449 - acc: 0.8251 - val_loss: 1.1777 - val_acc: 0.7590\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0394 - acc: 0.8285 - val_loss: 1.1749 - val_acc: 0.7570\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0342 - acc: 0.8289 - val_loss: 1.1696 - val_acc: 0.7570\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0287 - acc: 0.8332 - val_loss: 1.1666 - val_acc: 0.7580\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0233 - acc: 0.8324 - val_loss: 1.1628 - val_acc: 0.7640\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0177 - acc: 0.8344 - val_loss: 1.1596 - val_acc: 0.7620\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0128 - acc: 0.8339 - val_loss: 1.1588 - val_acc: 0.7590\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0078 - acc: 0.8377 - val_loss: 1.1545 - val_acc: 0.7670\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0027 - acc: 0.8363 - val_loss: 1.1526 - val_acc: 0.7580\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9977 - acc: 0.8405 - val_loss: 1.1481 - val_acc: 0.7640\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9923 - acc: 0.8393 - val_loss: 1.1454 - val_acc: 0.7620\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9881 - acc: 0.8392 - val_loss: 1.1419 - val_acc: 0.7650\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9831 - acc: 0.8412 - val_loss: 1.1403 - val_acc: 0.7610\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9778 - acc: 0.8463 - val_loss: 1.1379 - val_acc: 0.7670\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9736 - acc: 0.8460 - val_loss: 1.1357 - val_acc: 0.7610\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9691 - acc: 0.8464 - val_loss: 1.1320 - val_acc: 0.7620\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9640 - acc: 0.8488 - val_loss: 1.1308 - val_acc: 0.7620\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9595 - acc: 0.8475 - val_loss: 1.1316 - val_acc: 0.7580\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9550 - acc: 0.8536 - val_loss: 1.1240 - val_acc: 0.7700\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9506 - acc: 0.8504 - val_loss: 1.1225 - val_acc: 0.7650\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9464 - acc: 0.8531 - val_loss: 1.1207 - val_acc: 0.7680\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9417 - acc: 0.8536 - val_loss: 1.1177 - val_acc: 0.7680\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9373 - acc: 0.8567 - val_loss: 1.1159 - val_acc: 0.7690\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9330 - acc: 0.8577 - val_loss: 1.1131 - val_acc: 0.7740\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9289 - acc: 0.8568 - val_loss: 1.1103 - val_acc: 0.7700\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9248 - acc: 0.8587 - val_loss: 1.1085 - val_acc: 0.7720\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9207 - acc: 0.8592 - val_loss: 1.1068 - val_acc: 0.7690\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9165 - acc: 0.8613 - val_loss: 1.1045 - val_acc: 0.7690\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9123 - acc: 0.8633 - val_loss: 1.1018 - val_acc: 0.7700\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9084 - acc: 0.8619 - val_loss: 1.0995 - val_acc: 0.7740\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9046 - acc: 0.8620 - val_loss: 1.0973 - val_acc: 0.7740\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9005 - acc: 0.8620 - val_loss: 1.0970 - val_acc: 0.7730\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8963 - acc: 0.8645 - val_loss: 1.0944 - val_acc: 0.7750\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8925 - acc: 0.8648 - val_loss: 1.0915 - val_acc: 0.7720\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8885 - acc: 0.8665 - val_loss: 1.0903 - val_acc: 0.7750\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8847 - acc: 0.8652 - val_loss: 1.0893 - val_acc: 0.7700\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8811 - acc: 0.8693 - val_loss: 1.0863 - val_acc: 0.7750\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8777 - acc: 0.8688 - val_loss: 1.0860 - val_acc: 0.7710\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8735 - acc: 0.8692 - val_loss: 1.0838 - val_acc: 0.7700\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8701 - acc: 0.8695 - val_loss: 1.0803 - val_acc: 0.7720\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8661 - acc: 0.8717 - val_loss: 1.0793 - val_acc: 0.7750\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8625 - acc: 0.8724 - val_loss: 1.0773 - val_acc: 0.7720\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8590 - acc: 0.8745 - val_loss: 1.0742 - val_acc: 0.7730\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8553 - acc: 0.8740 - val_loss: 1.0726 - val_acc: 0.7740\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8521 - acc: 0.8757 - val_loss: 1.0712 - val_acc: 0.7730\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8484 - acc: 0.8759 - val_loss: 1.0696 - val_acc: 0.7730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8452 - acc: 0.8772 - val_loss: 1.0708 - val_acc: 0.7710\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 16.0272 - acc: 0.1337 - val_loss: 15.6019 - val_acc: 0.1770\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 15.2438 - acc: 0.2023 - val_loss: 14.8485 - val_acc: 0.2210\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 14.5064 - acc: 0.2331 - val_loss: 14.1235 - val_acc: 0.2480\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 13.7914 - acc: 0.2616 - val_loss: 13.4184 - val_acc: 0.2600\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 13.0959 - acc: 0.2831 - val_loss: 12.7353 - val_acc: 0.3070\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 12.4199 - acc: 0.3163 - val_loss: 12.0682 - val_acc: 0.3330\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 11.7632 - acc: 0.3444 - val_loss: 11.4224 - val_acc: 0.3590\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 11.1265 - acc: 0.3771 - val_loss: 10.7969 - val_acc: 0.3860\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 10.5102 - acc: 0.4095 - val_loss: 10.1924 - val_acc: 0.4200\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 9.9134 - acc: 0.4443 - val_loss: 9.6078 - val_acc: 0.4570\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 9.3369 - acc: 0.4721 - val_loss: 9.0438 - val_acc: 0.4810\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 8.7815 - acc: 0.4948 - val_loss: 8.5009 - val_acc: 0.5050\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 8.2476 - acc: 0.5159 - val_loss: 7.9796 - val_acc: 0.5150\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 7.7347 - acc: 0.5280 - val_loss: 7.4792 - val_acc: 0.5340\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 7.2440 - acc: 0.5405 - val_loss: 7.0023 - val_acc: 0.5410\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 6.7755 - acc: 0.5467 - val_loss: 6.5454 - val_acc: 0.5510\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 6.3293 - acc: 0.5569 - val_loss: 6.1099 - val_acc: 0.5620\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 5.9048 - acc: 0.5685 - val_loss: 5.6973 - val_acc: 0.5790\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 5.5030 - acc: 0.5753 - val_loss: 5.3075 - val_acc: 0.5920\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 5.1237 - acc: 0.5823 - val_loss: 4.9404 - val_acc: 0.5940\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 4.7682 - acc: 0.5867 - val_loss: 4.5961 - val_acc: 0.6130\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 4.4354 - acc: 0.5951 - val_loss: 4.2770 - val_acc: 0.6200\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 4.1261 - acc: 0.6035 - val_loss: 3.9777 - val_acc: 0.6270\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 3.8396 - acc: 0.6077 - val_loss: 3.7031 - val_acc: 0.6400\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 3.5758 - acc: 0.6172 - val_loss: 3.4538 - val_acc: 0.6390\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 3.3347 - acc: 0.6228 - val_loss: 3.2212 - val_acc: 0.6400\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 3.1157 - acc: 0.6232 - val_loss: 3.0145 - val_acc: 0.6490\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.9184 - acc: 0.6308 - val_loss: 2.8289 - val_acc: 0.6580\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.7424 - acc: 0.6395 - val_loss: 2.6599 - val_acc: 0.6550\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 2.5874 - acc: 0.6357 - val_loss: 2.5165 - val_acc: 0.6620\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 2.4530 - acc: 0.6403 - val_loss: 2.3908 - val_acc: 0.6560\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 2.3387 - acc: 0.6447 - val_loss: 2.2865 - val_acc: 0.6720\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 2.2440 - acc: 0.6473 - val_loss: 2.2004 - val_acc: 0.6660\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 2.1682 - acc: 0.6468 - val_loss: 2.1377 - val_acc: 0.6720\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 2.1095 - acc: 0.6484 - val_loss: 2.0817 - val_acc: 0.6690\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 2.0653 - acc: 0.6504 - val_loss: 2.0500 - val_acc: 0.6740\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.0326 - acc: 0.6533 - val_loss: 2.0151 - val_acc: 0.6730\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 2.0056 - acc: 0.6552 - val_loss: 1.9925 - val_acc: 0.6750\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.9830 - acc: 0.6591 - val_loss: 1.9684 - val_acc: 0.6860\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.9626 - acc: 0.6613 - val_loss: 1.9506 - val_acc: 0.6800\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.9434 - acc: 0.6681 - val_loss: 1.9299 - val_acc: 0.6740\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.9256 - acc: 0.6683 - val_loss: 1.9156 - val_acc: 0.6840\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.9088 - acc: 0.6695 - val_loss: 1.9000 - val_acc: 0.6780\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.8929 - acc: 0.6712 - val_loss: 1.8802 - val_acc: 0.6830\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.8770 - acc: 0.6720 - val_loss: 1.8636 - val_acc: 0.6830\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8621 - acc: 0.6763 - val_loss: 1.8524 - val_acc: 0.6910\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8478 - acc: 0.6793 - val_loss: 1.8371 - val_acc: 0.6860\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8346 - acc: 0.6793 - val_loss: 1.8216 - val_acc: 0.6790\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8213 - acc: 0.6803 - val_loss: 1.8128 - val_acc: 0.7030\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8090 - acc: 0.6815 - val_loss: 1.7975 - val_acc: 0.6870\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.7968 - acc: 0.6821 - val_loss: 1.7852 - val_acc: 0.6960\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.7843 - acc: 0.6835 - val_loss: 1.7738 - val_acc: 0.7010\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7730 - acc: 0.6851 - val_loss: 1.7619 - val_acc: 0.6930\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.7618 - acc: 0.6855 - val_loss: 1.7512 - val_acc: 0.6940\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.7508 - acc: 0.6856 - val_loss: 1.7490 - val_acc: 0.7030\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.7402 - acc: 0.6851 - val_loss: 1.7302 - val_acc: 0.7020\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.7297 - acc: 0.6863 - val_loss: 1.7221 - val_acc: 0.7040\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.7194 - acc: 0.6864 - val_loss: 1.7100 - val_acc: 0.7090\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7096 - acc: 0.6871 - val_loss: 1.7010 - val_acc: 0.7080\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.6996 - acc: 0.6901 - val_loss: 1.6951 - val_acc: 0.7040\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.6904 - acc: 0.6903 - val_loss: 1.6923 - val_acc: 0.7130\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.6813 - acc: 0.6892 - val_loss: 1.6716 - val_acc: 0.6990\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.6716 - acc: 0.6903 - val_loss: 1.6663 - val_acc: 0.7120\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.6622 - acc: 0.6921 - val_loss: 1.6519 - val_acc: 0.7010\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.6535 - acc: 0.6931 - val_loss: 1.6452 - val_acc: 0.7110\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.6452 - acc: 0.6919 - val_loss: 1.6358 - val_acc: 0.7100\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6368 - acc: 0.6936 - val_loss: 1.6287 - val_acc: 0.7050\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.6281 - acc: 0.6935 - val_loss: 1.6203 - val_acc: 0.7140\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.6197 - acc: 0.6939 - val_loss: 1.6115 - val_acc: 0.7100\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.6116 - acc: 0.6928 - val_loss: 1.6032 - val_acc: 0.7150\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.6039 - acc: 0.6948 - val_loss: 1.5990 - val_acc: 0.7100\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.5958 - acc: 0.6964 - val_loss: 1.5876 - val_acc: 0.7170\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5882 - acc: 0.6971 - val_loss: 1.5826 - val_acc: 0.7170\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5802 - acc: 0.6975 - val_loss: 1.5794 - val_acc: 0.7180\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.5731 - acc: 0.6959 - val_loss: 1.5638 - val_acc: 0.7180\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5656 - acc: 0.6981 - val_loss: 1.5568 - val_acc: 0.7160\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.5582 - acc: 0.6980 - val_loss: 1.5517 - val_acc: 0.7170\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5505 - acc: 0.6984 - val_loss: 1.5443 - val_acc: 0.7110\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5433 - acc: 0.6993 - val_loss: 1.5377 - val_acc: 0.7170\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5360 - acc: 0.7000 - val_loss: 1.5307 - val_acc: 0.7170\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.5284 - acc: 0.6992 - val_loss: 1.5228 - val_acc: 0.7220\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.5216 - acc: 0.7007 - val_loss: 1.5183 - val_acc: 0.7200\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.5146 - acc: 0.7008 - val_loss: 1.5090 - val_acc: 0.7200\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.5079 - acc: 0.7029 - val_loss: 1.4996 - val_acc: 0.7190\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.5007 - acc: 0.7040 - val_loss: 1.4969 - val_acc: 0.7230\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4945 - acc: 0.7032 - val_loss: 1.4877 - val_acc: 0.7200\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4879 - acc: 0.7033 - val_loss: 1.4808 - val_acc: 0.7210\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4812 - acc: 0.7039 - val_loss: 1.4768 - val_acc: 0.7230\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4747 - acc: 0.7047 - val_loss: 1.4703 - val_acc: 0.7200\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4684 - acc: 0.7037 - val_loss: 1.4617 - val_acc: 0.7220\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4615 - acc: 0.7045 - val_loss: 1.4572 - val_acc: 0.7210\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4550 - acc: 0.7076 - val_loss: 1.4499 - val_acc: 0.7200\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4495 - acc: 0.7051 - val_loss: 1.4426 - val_acc: 0.7240\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4431 - acc: 0.7071 - val_loss: 1.4367 - val_acc: 0.7230\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4366 - acc: 0.7095 - val_loss: 1.4314 - val_acc: 0.7260\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4304 - acc: 0.7088 - val_loss: 1.4384 - val_acc: 0.7110\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4247 - acc: 0.7111 - val_loss: 1.4215 - val_acc: 0.7240\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4181 - acc: 0.7104 - val_loss: 1.4136 - val_acc: 0.7250\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4126 - acc: 0.7109 - val_loss: 1.4091 - val_acc: 0.7260\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4064 - acc: 0.7100 - val_loss: 1.4069 - val_acc: 0.7210\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4017 - acc: 0.7112 - val_loss: 1.3982 - val_acc: 0.7250\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.3949 - acc: 0.7131 - val_loss: 1.3921 - val_acc: 0.7230\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3898 - acc: 0.7125 - val_loss: 1.3886 - val_acc: 0.7220\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.3839 - acc: 0.7141 - val_loss: 1.3849 - val_acc: 0.7160\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.3784 - acc: 0.7140 - val_loss: 1.3763 - val_acc: 0.7230\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.3734 - acc: 0.7136 - val_loss: 1.3717 - val_acc: 0.7230\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.3674 - acc: 0.7147 - val_loss: 1.3680 - val_acc: 0.7210\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.3619 - acc: 0.7151 - val_loss: 1.3596 - val_acc: 0.7270\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3567 - acc: 0.7151 - val_loss: 1.3579 - val_acc: 0.7230\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3513 - acc: 0.7152 - val_loss: 1.3498 - val_acc: 0.7280\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3459 - acc: 0.7164 - val_loss: 1.3435 - val_acc: 0.7290\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3405 - acc: 0.7177 - val_loss: 1.3456 - val_acc: 0.7260\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.3357 - acc: 0.7172 - val_loss: 1.3404 - val_acc: 0.7300\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3308 - acc: 0.7171 - val_loss: 1.3337 - val_acc: 0.7260\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.3250 - acc: 0.7180 - val_loss: 1.3242 - val_acc: 0.7290\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.3209 - acc: 0.7172 - val_loss: 1.3200 - val_acc: 0.7260\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3148 - acc: 0.7193 - val_loss: 1.3143 - val_acc: 0.7300\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.3106 - acc: 0.7204 - val_loss: 1.3115 - val_acc: 0.7300\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.3051 - acc: 0.7209 - val_loss: 1.3051 - val_acc: 0.7300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.3000 - acc: 0.7197 - val_loss: 1.3023 - val_acc: 0.7270\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 16.0207 - acc: 0.1465 - val_loss: 15.6192 - val_acc: 0.1660\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 15.2619 - acc: 0.1871 - val_loss: 14.8731 - val_acc: 0.2070\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 14.5274 - acc: 0.2165 - val_loss: 14.1498 - val_acc: 0.2260\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 13.8145 - acc: 0.2447 - val_loss: 13.4478 - val_acc: 0.2520\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 13.1221 - acc: 0.2676 - val_loss: 12.7661 - val_acc: 0.2740\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 12.4496 - acc: 0.2845 - val_loss: 12.1046 - val_acc: 0.3000\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 11.7972 - acc: 0.3083 - val_loss: 11.4638 - val_acc: 0.3100\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 11.1650 - acc: 0.3281 - val_loss: 10.8425 - val_acc: 0.3350\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 10.5525 - acc: 0.3581 - val_loss: 10.2411 - val_acc: 0.3650\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 9.9594 - acc: 0.3853 - val_loss: 9.6599 - val_acc: 0.4030\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 9.3854 - acc: 0.4120 - val_loss: 9.0971 - val_acc: 0.4320\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 8.8313 - acc: 0.4433 - val_loss: 8.5555 - val_acc: 0.4570\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 8.2983 - acc: 0.4796 - val_loss: 8.0346 - val_acc: 0.4870\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 7.7868 - acc: 0.5127 - val_loss: 7.5356 - val_acc: 0.5380\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 7.2967 - acc: 0.5516 - val_loss: 7.0572 - val_acc: 0.5660\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 6.8292 - acc: 0.5817 - val_loss: 6.6023 - val_acc: 0.5830\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 6.3844 - acc: 0.6075 - val_loss: 6.1687 - val_acc: 0.6100\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 5.9623 - acc: 0.6203 - val_loss: 5.7605 - val_acc: 0.6410\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 5.5626 - acc: 0.6473 - val_loss: 5.3712 - val_acc: 0.6400\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 5.1851 - acc: 0.6575 - val_loss: 5.0065 - val_acc: 0.6540\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 4.8305 - acc: 0.6660 - val_loss: 4.6628 - val_acc: 0.6630\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 4.4988 - acc: 0.6755 - val_loss: 4.3425 - val_acc: 0.6710\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 4.1904 - acc: 0.6817 - val_loss: 4.0459 - val_acc: 0.6720\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 3.9041 - acc: 0.6873 - val_loss: 3.7718 - val_acc: 0.6820\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 3.6407 - acc: 0.6871 - val_loss: 3.5187 - val_acc: 0.6910\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 3.3995 - acc: 0.6905 - val_loss: 3.2885 - val_acc: 0.7010\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 3.1796 - acc: 0.6931 - val_loss: 3.0801 - val_acc: 0.6980\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 2.9810 - acc: 0.6965 - val_loss: 2.8904 - val_acc: 0.7100\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.8031 - acc: 0.6964 - val_loss: 2.7228 - val_acc: 0.7020\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.6468 - acc: 0.6975 - val_loss: 2.5774 - val_acc: 0.7030\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 2.5111 - acc: 0.6992 - val_loss: 2.4520 - val_acc: 0.7140\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 2.3950 - acc: 0.6972 - val_loss: 2.3439 - val_acc: 0.7000\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.2982 - acc: 0.6975 - val_loss: 2.2563 - val_acc: 0.6990\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.2198 - acc: 0.6979 - val_loss: 2.1860 - val_acc: 0.6980\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.1586 - acc: 0.6964 - val_loss: 2.1308 - val_acc: 0.7010\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.1121 - acc: 0.6964 - val_loss: 2.0910 - val_acc: 0.7050\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 2.0766 - acc: 0.6967 - val_loss: 2.0598 - val_acc: 0.7020\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 2.0476 - acc: 0.6956 - val_loss: 2.0317 - val_acc: 0.7010\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 2.0224 - acc: 0.6940 - val_loss: 2.0090 - val_acc: 0.7030\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.9994 - acc: 0.6937 - val_loss: 1.9855 - val_acc: 0.6990\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.9791 - acc: 0.6924 - val_loss: 1.9641 - val_acc: 0.7040\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.9595 - acc: 0.6908 - val_loss: 1.9468 - val_acc: 0.6990\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.9414 - acc: 0.6925 - val_loss: 1.9282 - val_acc: 0.7050\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.9243 - acc: 0.6941 - val_loss: 1.9105 - val_acc: 0.6980\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.9082 - acc: 0.6941 - val_loss: 1.8975 - val_acc: 0.7070\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8924 - acc: 0.6937 - val_loss: 1.8831 - val_acc: 0.6990\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.8778 - acc: 0.6941 - val_loss: 1.8654 - val_acc: 0.7050\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.8634 - acc: 0.6936 - val_loss: 1.8519 - val_acc: 0.7000\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.8496 - acc: 0.6929 - val_loss: 1.8380 - val_acc: 0.7080\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8366 - acc: 0.6944 - val_loss: 1.8277 - val_acc: 0.7050\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.8240 - acc: 0.6935 - val_loss: 1.8139 - val_acc: 0.6990\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.8115 - acc: 0.6947 - val_loss: 1.8049 - val_acc: 0.7030\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8002 - acc: 0.6944 - val_loss: 1.7897 - val_acc: 0.6980\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.7884 - acc: 0.6936 - val_loss: 1.7800 - val_acc: 0.7070\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.7768 - acc: 0.6949 - val_loss: 1.7668 - val_acc: 0.7050\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.7663 - acc: 0.6979 - val_loss: 1.7561 - val_acc: 0.7060\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.7560 - acc: 0.6968 - val_loss: 1.7456 - val_acc: 0.7100\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.7455 - acc: 0.6964 - val_loss: 1.7356 - val_acc: 0.7070\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.7354 - acc: 0.6959 - val_loss: 1.7301 - val_acc: 0.7120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.7260 - acc: 0.6981 - val_loss: 1.7192 - val_acc: 0.7020\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.7166 - acc: 0.696 - 0s 32us/step - loss: 1.7165 - acc: 0.6961 - val_loss: 1.7060 - val_acc: 0.7080\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.7069 - acc: 0.6980 - val_loss: 1.6976 - val_acc: 0.7080\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.6975 - acc: 0.6996 - val_loss: 1.6871 - val_acc: 0.7110\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.6885 - acc: 0.6993 - val_loss: 1.6825 - val_acc: 0.7060\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.6800 - acc: 0.7007 - val_loss: 1.6726 - val_acc: 0.7090\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.6712 - acc: 0.7004 - val_loss: 1.6632 - val_acc: 0.7130\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.6628 - acc: 0.6992 - val_loss: 1.6544 - val_acc: 0.7140\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.6544 - acc: 0.7008 - val_loss: 1.6461 - val_acc: 0.7120\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.6460 - acc: 0.7005 - val_loss: 1.6392 - val_acc: 0.7160\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.6379 - acc: 0.7020 - val_loss: 1.6329 - val_acc: 0.7100\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.6300 - acc: 0.7016 - val_loss: 1.6228 - val_acc: 0.7140\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.6223 - acc: 0.7024 - val_loss: 1.6146 - val_acc: 0.7160\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.6142 - acc: 0.7007 - val_loss: 1.6096 - val_acc: 0.7200\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.6067 - acc: 0.7020 - val_loss: 1.6005 - val_acc: 0.7130\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5992 - acc: 0.7032 - val_loss: 1.5939 - val_acc: 0.7150\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.5913 - acc: 0.7040 - val_loss: 1.5867 - val_acc: 0.7160\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5841 - acc: 0.7036 - val_loss: 1.5783 - val_acc: 0.7200\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5769 - acc: 0.7048 - val_loss: 1.5706 - val_acc: 0.7190\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5695 - acc: 0.7055 - val_loss: 1.5651 - val_acc: 0.7170\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.5629 - acc: 0.7035 - val_loss: 1.5583 - val_acc: 0.7190\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.5556 - acc: 0.7055 - val_loss: 1.5519 - val_acc: 0.7170\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.5491 - acc: 0.7047 - val_loss: 1.5454 - val_acc: 0.7170\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.5418 - acc: 0.7068 - val_loss: 1.5394 - val_acc: 0.7210\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5354 - acc: 0.7064 - val_loss: 1.5339 - val_acc: 0.7200\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5283 - acc: 0.7069 - val_loss: 1.5307 - val_acc: 0.7170\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5224 - acc: 0.7091 - val_loss: 1.5200 - val_acc: 0.7180\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5153 - acc: 0.7079 - val_loss: 1.5132 - val_acc: 0.7190\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5087 - acc: 0.7091 - val_loss: 1.5086 - val_acc: 0.7270\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.5023 - acc: 0.7093 - val_loss: 1.5003 - val_acc: 0.7230\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4960 - acc: 0.7113 - val_loss: 1.4962 - val_acc: 0.7220\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.4897 - acc: 0.7100 - val_loss: 1.4907 - val_acc: 0.7180\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.4836 - acc: 0.7115 - val_loss: 1.4819 - val_acc: 0.7280\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.4773 - acc: 0.7107 - val_loss: 1.4754 - val_acc: 0.7200\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4708 - acc: 0.7112 - val_loss: 1.4751 - val_acc: 0.7200\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.4655 - acc: 0.7137 - val_loss: 1.4665 - val_acc: 0.7220\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.4595 - acc: 0.7112 - val_loss: 1.4614 - val_acc: 0.7240\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4534 - acc: 0.7115 - val_loss: 1.4559 - val_acc: 0.7180\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4477 - acc: 0.7129 - val_loss: 1.4470 - val_acc: 0.7230\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.4411 - acc: 0.7145 - val_loss: 1.4418 - val_acc: 0.7240\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4354 - acc: 0.7152 - val_loss: 1.4372 - val_acc: 0.7310\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.4303 - acc: 0.7155 - val_loss: 1.4307 - val_acc: 0.7250\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.4244 - acc: 0.7160 - val_loss: 1.4275 - val_acc: 0.7260\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.4191 - acc: 0.7143 - val_loss: 1.4230 - val_acc: 0.7260\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.4137 - acc: 0.7148 - val_loss: 1.4184 - val_acc: 0.7280\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.4077 - acc: 0.7156 - val_loss: 1.4114 - val_acc: 0.7270\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.4024 - acc: 0.7171 - val_loss: 1.4051 - val_acc: 0.7300\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3971 - acc: 0.7175 - val_loss: 1.4027 - val_acc: 0.7290\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.3920 - acc: 0.7184 - val_loss: 1.3960 - val_acc: 0.7260\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.3866 - acc: 0.7184 - val_loss: 1.3898 - val_acc: 0.7290\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3815 - acc: 0.7191 - val_loss: 1.3853 - val_acc: 0.7300\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3765 - acc: 0.7195 - val_loss: 1.3792 - val_acc: 0.7270\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3711 - acc: 0.7191 - val_loss: 1.3758 - val_acc: 0.7320\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3661 - acc: 0.7196 - val_loss: 1.3720 - val_acc: 0.7280\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3611 - acc: 0.7197 - val_loss: 1.3656 - val_acc: 0.7290\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3557 - acc: 0.7201 - val_loss: 1.3634 - val_acc: 0.7260\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.3508 - acc: 0.7200 - val_loss: 1.3632 - val_acc: 0.7270\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3459 - acc: 0.7219 - val_loss: 1.3513 - val_acc: 0.7320\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.3413 - acc: 0.7193 - val_loss: 1.3521 - val_acc: 0.7280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.3362 - acc: 0.7200 - val_loss: 1.3426 - val_acc: 0.7330\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.3310 - acc: 0.7209 - val_loss: 1.3355 - val_acc: 0.7320\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.3261 - acc: 0.7233 - val_loss: 1.3353 - val_acc: 0.7340\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.3221 - acc: 0.7221 - val_loss: 1.3295 - val_acc: 0.7360\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.3168 - acc: 0.7241 - val_loss: 1.3224 - val_acc: 0.7350\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.3121 - acc: 0.7245 - val_loss: 1.3211 - val_acc: 0.7320\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.3082 - acc: 0.7231 - val_loss: 1.3155 - val_acc: 0.7350\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.3032 - acc: 0.7245 - val_loss: 1.3130 - val_acc: 0.7360\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2990 - acc: 0.7244 - val_loss: 1.3077 - val_acc: 0.7360\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.2945 - acc: 0.7253 - val_loss: 1.3059 - val_acc: 0.7350\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2904 - acc: 0.7259 - val_loss: 1.3032 - val_acc: 0.7370\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.2864 - acc: 0.7237 - val_loss: 1.2975 - val_acc: 0.7350\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.2816 - acc: 0.7267 - val_loss: 1.2932 - val_acc: 0.7290\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2780 - acc: 0.7257 - val_loss: 1.2858 - val_acc: 0.7410\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.2728 - acc: 0.7260 - val_loss: 1.2832 - val_acc: 0.7400\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.2690 - acc: 0.7265 - val_loss: 1.2780 - val_acc: 0.7440\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.2647 - acc: 0.7299 - val_loss: 1.2759 - val_acc: 0.7380\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.2608 - acc: 0.7297 - val_loss: 1.2773 - val_acc: 0.7380\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.2570 - acc: 0.7285 - val_loss: 1.2672 - val_acc: 0.7430\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.2532 - acc: 0.7297 - val_loss: 1.2663 - val_acc: 0.7420\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.2501 - acc: 0.7291 - val_loss: 1.2600 - val_acc: 0.7460\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.2454 - acc: 0.7301 - val_loss: 1.2551 - val_acc: 0.7440\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.2415 - acc: 0.7315 - val_loss: 1.2526 - val_acc: 0.7420\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.2378 - acc: 0.7301 - val_loss: 1.2505 - val_acc: 0.7460\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.2340 - acc: 0.7307 - val_loss: 1.2471 - val_acc: 0.7390\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.2304 - acc: 0.7321 - val_loss: 1.2455 - val_acc: 0.7390\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.2272 - acc: 0.7323 - val_loss: 1.2393 - val_acc: 0.7420\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2229 - acc: 0.7328 - val_loss: 1.2396 - val_acc: 0.7390\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2196 - acc: 0.7319 - val_loss: 1.2312 - val_acc: 0.7470\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.2159 - acc: 0.7317 - val_loss: 1.2307 - val_acc: 0.7450\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2121 - acc: 0.7328 - val_loss: 1.2351 - val_acc: 0.7340\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.2097 - acc: 0.7348 - val_loss: 1.2234 - val_acc: 0.7410\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.2054 - acc: 0.7329 - val_loss: 1.2218 - val_acc: 0.7480\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.2026 - acc: 0.7337 - val_loss: 1.2222 - val_acc: 0.7400\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1992 - acc: 0.7353 - val_loss: 1.2136 - val_acc: 0.7460\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.1960 - acc: 0.7332 - val_loss: 1.2094 - val_acc: 0.7490\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.1920 - acc: 0.7357 - val_loss: 1.2158 - val_acc: 0.7410\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1893 - acc: 0.7360 - val_loss: 1.2026 - val_acc: 0.7470\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1862 - acc: 0.7359 - val_loss: 1.2018 - val_acc: 0.7440\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1827 - acc: 0.7355 - val_loss: 1.1992 - val_acc: 0.7510\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1797 - acc: 0.7371 - val_loss: 1.1995 - val_acc: 0.7470\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1772 - acc: 0.7368 - val_loss: 1.1959 - val_acc: 0.7450\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1740 - acc: 0.7356 - val_loss: 1.1961 - val_acc: 0.7400\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1710 - acc: 0.7375 - val_loss: 1.1916 - val_acc: 0.7410\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1682 - acc: 0.7372 - val_loss: 1.1843 - val_acc: 0.7470\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.1655 - acc: 0.7368 - val_loss: 1.1834 - val_acc: 0.7460\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1627 - acc: 0.7379 - val_loss: 1.1821 - val_acc: 0.7500\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1601 - acc: 0.7388 - val_loss: 1.1785 - val_acc: 0.7520\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1580 - acc: 0.7383 - val_loss: 1.1756 - val_acc: 0.7490\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1549 - acc: 0.7391 - val_loss: 1.1752 - val_acc: 0.7520\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.1524 - acc: 0.7399 - val_loss: 1.1787 - val_acc: 0.7450\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1507 - acc: 0.7375 - val_loss: 1.1682 - val_acc: 0.7480\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.1472 - acc: 0.7385 - val_loss: 1.1695 - val_acc: 0.7460\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1450 - acc: 0.7387 - val_loss: 1.1656 - val_acc: 0.7500\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.1426 - acc: 0.7405 - val_loss: 1.1659 - val_acc: 0.7500\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.1403 - acc: 0.7400 - val_loss: 1.1602 - val_acc: 0.7470\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.1380 - acc: 0.7409 - val_loss: 1.1569 - val_acc: 0.7460\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.1355 - acc: 0.7420 - val_loss: 1.1554 - val_acc: 0.7480\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1327 - acc: 0.7415 - val_loss: 1.1516 - val_acc: 0.7530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.1310 - acc: 0.7400 - val_loss: 1.1487 - val_acc: 0.7510\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.1283 - acc: 0.7409 - val_loss: 1.1513 - val_acc: 0.7470\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1262 - acc: 0.7413 - val_loss: 1.1493 - val_acc: 0.7520\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.1242 - acc: 0.7412 - val_loss: 1.1439 - val_acc: 0.7500\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.1220 - acc: 0.7429 - val_loss: 1.1473 - val_acc: 0.7500\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1204 - acc: 0.7424 - val_loss: 1.1442 - val_acc: 0.7480\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.1184 - acc: 0.7435 - val_loss: 1.1447 - val_acc: 0.7470\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.1163 - acc: 0.7425 - val_loss: 1.1399 - val_acc: 0.7530\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1145 - acc: 0.7427 - val_loss: 1.1370 - val_acc: 0.7440\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1119 - acc: 0.7429 - val_loss: 1.1370 - val_acc: 0.7440\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1104 - acc: 0.7437 - val_loss: 1.1357 - val_acc: 0.7480\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.1090 - acc: 0.7431 - val_loss: 1.1306 - val_acc: 0.7460\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.1066 - acc: 0.7452 - val_loss: 1.1299 - val_acc: 0.7500\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1050 - acc: 0.7445 - val_loss: 1.1306 - val_acc: 0.7530\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1029 - acc: 0.7451 - val_loss: 1.1303 - val_acc: 0.7520\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1013 - acc: 0.7459 - val_loss: 1.1246 - val_acc: 0.7540\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0993 - acc: 0.7444 - val_loss: 1.1230 - val_acc: 0.7530\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0972 - acc: 0.7456 - val_loss: 1.1241 - val_acc: 0.7520\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0959 - acc: 0.7465 - val_loss: 1.1201 - val_acc: 0.7470\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0947 - acc: 0.7444 - val_loss: 1.1192 - val_acc: 0.7490\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0925 - acc: 0.7467 - val_loss: 1.1166 - val_acc: 0.7470\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0913 - acc: 0.7452 - val_loss: 1.1171 - val_acc: 0.7540\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0895 - acc: 0.7457 - val_loss: 1.1168 - val_acc: 0.7570\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0875 - acc: 0.7469 - val_loss: 1.1124 - val_acc: 0.7470\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0859 - acc: 0.7464 - val_loss: 1.1109 - val_acc: 0.7550\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0844 - acc: 0.7487 - val_loss: 1.1092 - val_acc: 0.7560\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0826 - acc: 0.7472 - val_loss: 1.1130 - val_acc: 0.7450\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0810 - acc: 0.7465 - val_loss: 1.1133 - val_acc: 0.7500\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0797 - acc: 0.7485 - val_loss: 1.1072 - val_acc: 0.7520\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0784 - acc: 0.7483 - val_loss: 1.1142 - val_acc: 0.7390\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0765 - acc: 0.7473 - val_loss: 1.1057 - val_acc: 0.7550\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0754 - acc: 0.7479 - val_loss: 1.1083 - val_acc: 0.7500\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0735 - acc: 0.7489 - val_loss: 1.0997 - val_acc: 0.7540\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0718 - acc: 0.7497 - val_loss: 1.0978 - val_acc: 0.7550\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0702 - acc: 0.7481 - val_loss: 1.0996 - val_acc: 0.7520\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0688 - acc: 0.7496 - val_loss: 1.0976 - val_acc: 0.7510\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0681 - acc: 0.7483 - val_loss: 1.0970 - val_acc: 0.7480\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0660 - acc: 0.7504 - val_loss: 1.0971 - val_acc: 0.7470\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0643 - acc: 0.7488 - val_loss: 1.0944 - val_acc: 0.7540\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0637 - acc: 0.7491 - val_loss: 1.0916 - val_acc: 0.7490\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0616 - acc: 0.7503 - val_loss: 1.0898 - val_acc: 0.7520\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0600 - acc: 0.7484 - val_loss: 1.0900 - val_acc: 0.7470\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0587 - acc: 0.7499 - val_loss: 1.0919 - val_acc: 0.7520\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.0583 - acc: 0.7496 - val_loss: 1.0893 - val_acc: 0.7490\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0566 - acc: 0.7505 - val_loss: 1.0865 - val_acc: 0.7560\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0550 - acc: 0.7524 - val_loss: 1.0869 - val_acc: 0.7470\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0537 - acc: 0.7515 - val_loss: 1.0840 - val_acc: 0.7510\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0527 - acc: 0.7513 - val_loss: 1.0832 - val_acc: 0.7550\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.0509 - acc: 0.7509 - val_loss: 1.0797 - val_acc: 0.7540\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.0499 - acc: 0.7509 - val_loss: 1.0840 - val_acc: 0.7440\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0485 - acc: 0.7513 - val_loss: 1.0805 - val_acc: 0.7470\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0476 - acc: 0.7521 - val_loss: 1.0792 - val_acc: 0.7460\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0459 - acc: 0.7527 - val_loss: 1.0806 - val_acc: 0.7490\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0452 - acc: 0.7525 - val_loss: 1.0755 - val_acc: 0.7550\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.0435 - acc: 0.7533 - val_loss: 1.0816 - val_acc: 0.7440\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0423 - acc: 0.7532 - val_loss: 1.0724 - val_acc: 0.7520\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0414 - acc: 0.7541 - val_loss: 1.0719 - val_acc: 0.7530\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0403 - acc: 0.7527 - val_loss: 1.0725 - val_acc: 0.7530\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0398 - acc: 0.7532 - val_loss: 1.0713 - val_acc: 0.7550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0382 - acc: 0.7536 - val_loss: 1.0699 - val_acc: 0.7560\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0365 - acc: 0.7528 - val_loss: 1.0668 - val_acc: 0.7510\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0357 - acc: 0.7541 - val_loss: 1.0701 - val_acc: 0.7520\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0346 - acc: 0.7543 - val_loss: 1.0666 - val_acc: 0.7450\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0335 - acc: 0.7539 - val_loss: 1.0636 - val_acc: 0.7550\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0320 - acc: 0.7523 - val_loss: 1.0643 - val_acc: 0.7590\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0318 - acc: 0.7545 - val_loss: 1.0619 - val_acc: 0.7530\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0295 - acc: 0.7543 - val_loss: 1.0627 - val_acc: 0.7470\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0283 - acc: 0.7532 - val_loss: 1.0621 - val_acc: 0.7550\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0278 - acc: 0.7551 - val_loss: 1.0715 - val_acc: 0.7480\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0269 - acc: 0.7515 - val_loss: 1.0605 - val_acc: 0.7580\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0252 - acc: 0.7547 - val_loss: 1.0591 - val_acc: 0.7500\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0244 - acc: 0.7548 - val_loss: 1.0572 - val_acc: 0.7540\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0234 - acc: 0.7548 - val_loss: 1.0561 - val_acc: 0.7580\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0224 - acc: 0.7551 - val_loss: 1.0560 - val_acc: 0.7570\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0210 - acc: 0.7560 - val_loss: 1.0657 - val_acc: 0.7490\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0210 - acc: 0.7537 - val_loss: 1.0637 - val_acc: 0.7530\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0202 - acc: 0.7555 - val_loss: 1.0527 - val_acc: 0.7540\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0187 - acc: 0.7545 - val_loss: 1.0632 - val_acc: 0.7490\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0175 - acc: 0.7549 - val_loss: 1.0529 - val_acc: 0.7490\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0162 - acc: 0.7560 - val_loss: 1.0537 - val_acc: 0.7470\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0153 - acc: 0.7560 - val_loss: 1.0500 - val_acc: 0.7550\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0135 - acc: 0.7557 - val_loss: 1.0497 - val_acc: 0.7560\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0132 - acc: 0.7555 - val_loss: 1.0557 - val_acc: 0.7440\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0124 - acc: 0.7564 - val_loss: 1.0461 - val_acc: 0.7590\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0112 - acc: 0.7557 - val_loss: 1.0448 - val_acc: 0.7510\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0103 - acc: 0.7555 - val_loss: 1.0446 - val_acc: 0.7550\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0091 - acc: 0.7560 - val_loss: 1.0486 - val_acc: 0.7550\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0083 - acc: 0.7560 - val_loss: 1.0506 - val_acc: 0.7520\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0077 - acc: 0.7571 - val_loss: 1.0438 - val_acc: 0.7560\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0063 - acc: 0.7564 - val_loss: 1.0443 - val_acc: 0.7550\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0058 - acc: 0.7564 - val_loss: 1.0459 - val_acc: 0.7560\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 1.0050 - acc: 0.7552 - val_loss: 1.0445 - val_acc: 0.7480\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.0039 - acc: 0.7571 - val_loss: 1.0406 - val_acc: 0.7560\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0022 - acc: 0.7571 - val_loss: 1.0411 - val_acc: 0.7440\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.0019 - acc: 0.7569 - val_loss: 1.0384 - val_acc: 0.7520\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.0009 - acc: 0.7567 - val_loss: 1.0372 - val_acc: 0.7500\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.9993 - acc: 0.7559 - val_loss: 1.0370 - val_acc: 0.7500\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9997 - acc: 0.7563 - val_loss: 1.0340 - val_acc: 0.7530\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9988 - acc: 0.7571 - val_loss: 1.0349 - val_acc: 0.7550\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9974 - acc: 0.7575 - val_loss: 1.0401 - val_acc: 0.7530\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9961 - acc: 0.7576 - val_loss: 1.0322 - val_acc: 0.7560\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9958 - acc: 0.7569 - val_loss: 1.0346 - val_acc: 0.7500\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9948 - acc: 0.7575 - val_loss: 1.0351 - val_acc: 0.7590\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9936 - acc: 0.7589 - val_loss: 1.0303 - val_acc: 0.7580\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9934 - acc: 0.7571 - val_loss: 1.0328 - val_acc: 0.7520\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9919 - acc: 0.7584 - val_loss: 1.0299 - val_acc: 0.7530\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9908 - acc: 0.7584 - val_loss: 1.0323 - val_acc: 0.7520\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9907 - acc: 0.7577 - val_loss: 1.0307 - val_acc: 0.7470\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9899 - acc: 0.7583 - val_loss: 1.0295 - val_acc: 0.7590\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9884 - acc: 0.7579 - val_loss: 1.0256 - val_acc: 0.7570\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9884 - acc: 0.7584 - val_loss: 1.0242 - val_acc: 0.7540\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9865 - acc: 0.7595 - val_loss: 1.0245 - val_acc: 0.7570\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9858 - acc: 0.7587 - val_loss: 1.0238 - val_acc: 0.7540\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9852 - acc: 0.7589 - val_loss: 1.0228 - val_acc: 0.7610\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9840 - acc: 0.7596 - val_loss: 1.0266 - val_acc: 0.7590\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.9843 - acc: 0.7577 - val_loss: 1.0266 - val_acc: 0.7530\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9835 - acc: 0.7573 - val_loss: 1.0215 - val_acc: 0.7550\n",
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9818 - acc: 0.7604 - val_loss: 1.0236 - val_acc: 0.7480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9818 - acc: 0.7593 - val_loss: 1.0258 - val_acc: 0.7530\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9813 - acc: 0.7597 - val_loss: 1.0224 - val_acc: 0.7500\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9797 - acc: 0.7593 - val_loss: 1.0194 - val_acc: 0.7520\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9789 - acc: 0.7592 - val_loss: 1.0183 - val_acc: 0.7510\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9788 - acc: 0.7600 - val_loss: 1.0179 - val_acc: 0.7600\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9772 - acc: 0.7596 - val_loss: 1.0179 - val_acc: 0.7520\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9765 - acc: 0.7607 - val_loss: 1.0225 - val_acc: 0.7500\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9763 - acc: 0.7595 - val_loss: 1.0161 - val_acc: 0.7610\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9754 - acc: 0.7612 - val_loss: 1.0179 - val_acc: 0.7550\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9744 - acc: 0.7589 - val_loss: 1.0228 - val_acc: 0.7540\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9740 - acc: 0.7613 - val_loss: 1.0302 - val_acc: 0.7510\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9744 - acc: 0.7595 - val_loss: 1.0170 - val_acc: 0.7480\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9723 - acc: 0.7609 - val_loss: 1.0124 - val_acc: 0.7530\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9721 - acc: 0.7613 - val_loss: 1.0135 - val_acc: 0.7510\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9703 - acc: 0.7611 - val_loss: 1.0140 - val_acc: 0.7520\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9701 - acc: 0.7613 - val_loss: 1.0120 - val_acc: 0.7580\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9696 - acc: 0.7604 - val_loss: 1.0143 - val_acc: 0.7510\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9694 - acc: 0.7604 - val_loss: 1.0117 - val_acc: 0.7580\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9687 - acc: 0.7608 - val_loss: 1.0139 - val_acc: 0.7530\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9679 - acc: 0.7608 - val_loss: 1.0113 - val_acc: 0.7500\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9669 - acc: 0.7608 - val_loss: 1.0076 - val_acc: 0.7570\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9662 - acc: 0.7615 - val_loss: 1.0080 - val_acc: 0.7520\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9656 - acc: 0.7596 - val_loss: 1.0107 - val_acc: 0.7510\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9642 - acc: 0.7613 - val_loss: 1.0084 - val_acc: 0.7510\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9641 - acc: 0.7615 - val_loss: 1.0067 - val_acc: 0.7550\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9633 - acc: 0.7615 - val_loss: 1.0061 - val_acc: 0.7530\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9622 - acc: 0.7625 - val_loss: 1.0066 - val_acc: 0.7590\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9614 - acc: 0.7616 - val_loss: 1.0064 - val_acc: 0.7500\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9622 - acc: 0.7619 - val_loss: 1.0064 - val_acc: 0.7580\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9600 - acc: 0.7603 - val_loss: 1.0110 - val_acc: 0.7520\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9599 - acc: 0.7619 - val_loss: 1.0077 - val_acc: 0.7560\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9596 - acc: 0.7625 - val_loss: 1.0123 - val_acc: 0.7530\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9578 - acc: 0.7611 - val_loss: 1.0064 - val_acc: 0.7510\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9578 - acc: 0.7615 - val_loss: 1.0022 - val_acc: 0.7500\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9564 - acc: 0.7612 - val_loss: 1.0004 - val_acc: 0.7550\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9560 - acc: 0.7627 - val_loss: 1.0017 - val_acc: 0.7560\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9560 - acc: 0.7616 - val_loss: 0.9995 - val_acc: 0.7550\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9556 - acc: 0.7640 - val_loss: 1.0039 - val_acc: 0.7510\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9551 - acc: 0.7608 - val_loss: 1.0026 - val_acc: 0.7540\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9549 - acc: 0.7612 - val_loss: 0.9967 - val_acc: 0.7570\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9536 - acc: 0.7611 - val_loss: 0.9972 - val_acc: 0.7550\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9524 - acc: 0.7628 - val_loss: 1.0024 - val_acc: 0.7520\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9518 - acc: 0.7628 - val_loss: 0.9988 - val_acc: 0.7520\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9515 - acc: 0.7633 - val_loss: 0.9963 - val_acc: 0.7510\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9514 - acc: 0.7624 - val_loss: 1.0061 - val_acc: 0.7530\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9512 - acc: 0.7617 - val_loss: 1.0017 - val_acc: 0.7520\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9497 - acc: 0.7629 - val_loss: 0.9955 - val_acc: 0.7530\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9488 - acc: 0.7628 - val_loss: 0.9954 - val_acc: 0.7560\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9486 - acc: 0.7628 - val_loss: 0.9965 - val_acc: 0.7490\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9488 - acc: 0.7643 - val_loss: 1.0055 - val_acc: 0.7530\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9478 - acc: 0.7620 - val_loss: 0.9960 - val_acc: 0.7510\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9468 - acc: 0.7624 - val_loss: 0.9923 - val_acc: 0.7550\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9469 - acc: 0.7628 - val_loss: 0.9920 - val_acc: 0.7600\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9463 - acc: 0.7635 - val_loss: 1.0127 - val_acc: 0.7440\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9472 - acc: 0.7635 - val_loss: 0.9978 - val_acc: 0.7500\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9448 - acc: 0.7627 - val_loss: 0.9916 - val_acc: 0.7530\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9441 - acc: 0.7639 - val_loss: 0.9925 - val_acc: 0.7550\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9432 - acc: 0.7633 - val_loss: 0.9946 - val_acc: 0.7610\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9422 - acc: 0.7640 - val_loss: 0.9945 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 355/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9430 - acc: 0.7647 - val_loss: 0.9951 - val_acc: 0.7470\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9418 - acc: 0.7644 - val_loss: 0.9922 - val_acc: 0.7540\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9409 - acc: 0.7647 - val_loss: 0.9940 - val_acc: 0.7570\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9405 - acc: 0.7648 - val_loss: 0.9957 - val_acc: 0.7540\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9393 - acc: 0.7633 - val_loss: 0.9985 - val_acc: 0.7540\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9397 - acc: 0.7617 - val_loss: 0.9898 - val_acc: 0.7490\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9383 - acc: 0.7645 - val_loss: 0.9893 - val_acc: 0.7480\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9382 - acc: 0.7648 - val_loss: 0.9881 - val_acc: 0.7530\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9375 - acc: 0.7643 - val_loss: 0.9881 - val_acc: 0.7540\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9380 - acc: 0.7647 - val_loss: 0.9868 - val_acc: 0.7530\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9361 - acc: 0.7639 - val_loss: 0.9837 - val_acc: 0.7490\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9360 - acc: 0.7651 - val_loss: 0.9860 - val_acc: 0.7570\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9358 - acc: 0.7627 - val_loss: 0.9884 - val_acc: 0.7590\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9352 - acc: 0.7653 - val_loss: 0.9856 - val_acc: 0.7580\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9344 - acc: 0.7665 - val_loss: 0.9815 - val_acc: 0.7530\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9333 - acc: 0.7649 - val_loss: 0.9865 - val_acc: 0.7550\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9331 - acc: 0.7659 - val_loss: 0.9825 - val_acc: 0.7550\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9326 - acc: 0.7652 - val_loss: 0.9838 - val_acc: 0.7490\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9320 - acc: 0.7639 - val_loss: 0.9821 - val_acc: 0.7500\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9317 - acc: 0.7643 - val_loss: 0.9849 - val_acc: 0.7540\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9315 - acc: 0.7640 - val_loss: 0.9856 - val_acc: 0.7480\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9303 - acc: 0.7669 - val_loss: 0.9826 - val_acc: 0.7500\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9305 - acc: 0.7653 - val_loss: 0.9828 - val_acc: 0.7500\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9299 - acc: 0.7655 - val_loss: 0.9907 - val_acc: 0.7540\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9303 - acc: 0.7644 - val_loss: 0.9929 - val_acc: 0.7530\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9293 - acc: 0.7659 - val_loss: 0.9883 - val_acc: 0.7510\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9294 - acc: 0.7627 - val_loss: 0.9785 - val_acc: 0.7510\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9277 - acc: 0.7667 - val_loss: 0.9848 - val_acc: 0.7500\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9274 - acc: 0.7648 - val_loss: 0.9772 - val_acc: 0.7510\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9263 - acc: 0.7675 - val_loss: 0.9768 - val_acc: 0.7510\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9267 - acc: 0.7656 - val_loss: 0.9774 - val_acc: 0.7590\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9263 - acc: 0.7667 - val_loss: 0.9852 - val_acc: 0.7470\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9251 - acc: 0.7655 - val_loss: 0.9874 - val_acc: 0.7540\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9259 - acc: 0.7675 - val_loss: 0.9756 - val_acc: 0.7530\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9235 - acc: 0.7652 - val_loss: 0.9746 - val_acc: 0.7530\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9238 - acc: 0.7653 - val_loss: 0.9732 - val_acc: 0.7530\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9237 - acc: 0.7672 - val_loss: 0.9753 - val_acc: 0.7510\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9228 - acc: 0.7651 - val_loss: 0.9738 - val_acc: 0.7610\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9226 - acc: 0.7684 - val_loss: 0.9723 - val_acc: 0.7580\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9221 - acc: 0.7649 - val_loss: 0.9788 - val_acc: 0.7550\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9216 - acc: 0.7667 - val_loss: 0.9788 - val_acc: 0.7450\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9209 - acc: 0.7672 - val_loss: 0.9746 - val_acc: 0.7570\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9199 - acc: 0.7676 - val_loss: 0.9855 - val_acc: 0.7570\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9209 - acc: 0.7675 - val_loss: 0.9820 - val_acc: 0.7490\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9195 - acc: 0.7656 - val_loss: 0.9720 - val_acc: 0.7520\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9191 - acc: 0.7664 - val_loss: 0.9739 - val_acc: 0.7480\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9190 - acc: 0.7637 - val_loss: 0.9757 - val_acc: 0.7510\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9181 - acc: 0.7659 - val_loss: 0.9724 - val_acc: 0.7480\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9177 - acc: 0.7673 - val_loss: 0.9828 - val_acc: 0.7500\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9188 - acc: 0.7657 - val_loss: 0.9726 - val_acc: 0.7530\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9161 - acc: 0.7657 - val_loss: 0.9762 - val_acc: 0.7550\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9160 - acc: 0.7679 - val_loss: 0.9723 - val_acc: 0.7520\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9154 - acc: 0.7672 - val_loss: 0.9685 - val_acc: 0.7510\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9160 - acc: 0.7661 - val_loss: 0.9706 - val_acc: 0.7580\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9143 - acc: 0.7680 - val_loss: 0.9795 - val_acc: 0.7520\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9146 - acc: 0.7661 - val_loss: 0.9722 - val_acc: 0.7570\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9144 - acc: 0.7659 - val_loss: 0.9723 - val_acc: 0.7530\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9131 - acc: 0.7672 - val_loss: 0.9752 - val_acc: 0.7460\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9132 - acc: 0.7676 - val_loss: 0.9674 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 414/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9123 - acc: 0.7668 - val_loss: 0.9651 - val_acc: 0.7570\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9123 - acc: 0.7672 - val_loss: 0.9687 - val_acc: 0.7580\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9108 - acc: 0.7664 - val_loss: 0.9778 - val_acc: 0.7520\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9113 - acc: 0.7659 - val_loss: 0.9654 - val_acc: 0.7560\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9114 - acc: 0.7664 - val_loss: 0.9666 - val_acc: 0.7510\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9097 - acc: 0.7675 - val_loss: 0.9805 - val_acc: 0.7540\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9107 - acc: 0.7681 - val_loss: 0.9720 - val_acc: 0.7500\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9102 - acc: 0.7676 - val_loss: 0.9660 - val_acc: 0.7480\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9085 - acc: 0.7683 - val_loss: 0.9669 - val_acc: 0.7530\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9083 - acc: 0.7669 - val_loss: 0.9667 - val_acc: 0.7490\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9074 - acc: 0.7675 - val_loss: 0.9689 - val_acc: 0.7550\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9070 - acc: 0.7684 - val_loss: 0.9702 - val_acc: 0.7500\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9075 - acc: 0.7684 - val_loss: 0.9679 - val_acc: 0.7570\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9070 - acc: 0.7680 - val_loss: 0.9646 - val_acc: 0.7500\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9062 - acc: 0.7676 - val_loss: 0.9673 - val_acc: 0.7490\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9062 - acc: 0.7695 - val_loss: 0.9705 - val_acc: 0.7480\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9059 - acc: 0.7675 - val_loss: 0.9770 - val_acc: 0.7560\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9057 - acc: 0.7673 - val_loss: 0.9603 - val_acc: 0.7480\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9041 - acc: 0.7685 - val_loss: 0.9607 - val_acc: 0.7590\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9049 - acc: 0.7687 - val_loss: 0.9738 - val_acc: 0.7500\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9044 - acc: 0.7687 - val_loss: 0.9598 - val_acc: 0.7550\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9029 - acc: 0.7689 - val_loss: 0.9626 - val_acc: 0.7520\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9035 - acc: 0.7697 - val_loss: 0.9622 - val_acc: 0.7540\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9034 - acc: 0.7688 - val_loss: 0.9659 - val_acc: 0.7540\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9020 - acc: 0.7677 - val_loss: 0.9695 - val_acc: 0.7520\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9036 - acc: 0.7687 - val_loss: 0.9671 - val_acc: 0.7510\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9029 - acc: 0.7681 - val_loss: 0.9595 - val_acc: 0.7540\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9013 - acc: 0.7693 - val_loss: 0.9657 - val_acc: 0.7540\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9012 - acc: 0.7693 - val_loss: 0.9677 - val_acc: 0.7480\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9010 - acc: 0.7691 - val_loss: 0.9568 - val_acc: 0.7520\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8999 - acc: 0.7663 - val_loss: 0.9630 - val_acc: 0.7490\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9003 - acc: 0.7683 - val_loss: 0.9580 - val_acc: 0.7500\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8995 - acc: 0.7677 - val_loss: 0.9589 - val_acc: 0.7550\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8995 - acc: 0.7672 - val_loss: 0.9635 - val_acc: 0.7520\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8989 - acc: 0.7699 - val_loss: 0.9614 - val_acc: 0.7530\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8977 - acc: 0.7675 - val_loss: 0.9566 - val_acc: 0.7480\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8979 - acc: 0.7689 - val_loss: 0.9576 - val_acc: 0.7630\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8983 - acc: 0.7697 - val_loss: 0.9567 - val_acc: 0.7580\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8979 - acc: 0.7700 - val_loss: 0.9570 - val_acc: 0.7580\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8969 - acc: 0.7693 - val_loss: 0.9590 - val_acc: 0.7520\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8970 - acc: 0.7711 - val_loss: 0.9543 - val_acc: 0.7530\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8964 - acc: 0.7701 - val_loss: 0.9538 - val_acc: 0.7550\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8955 - acc: 0.7708 - val_loss: 0.9585 - val_acc: 0.7510\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8948 - acc: 0.7719 - val_loss: 0.9661 - val_acc: 0.7530\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8961 - acc: 0.7685 - val_loss: 0.9554 - val_acc: 0.7600\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8957 - acc: 0.7692 - val_loss: 0.9519 - val_acc: 0.7510\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8934 - acc: 0.7712 - val_loss: 0.9569 - val_acc: 0.7520\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8939 - acc: 0.7699 - val_loss: 0.9564 - val_acc: 0.7600\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8927 - acc: 0.7707 - val_loss: 0.9553 - val_acc: 0.7520\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8940 - acc: 0.7709 - val_loss: 0.9526 - val_acc: 0.7500\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8947 - acc: 0.7697 - val_loss: 0.9550 - val_acc: 0.7540\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8929 - acc: 0.7713 - val_loss: 0.9555 - val_acc: 0.7480\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8922 - acc: 0.7727 - val_loss: 0.9540 - val_acc: 0.7520\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8916 - acc: 0.7708 - val_loss: 0.9605 - val_acc: 0.7520\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8913 - acc: 0.7707 - val_loss: 0.9517 - val_acc: 0.7570\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8912 - acc: 0.7712 - val_loss: 0.9570 - val_acc: 0.7500\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8911 - acc: 0.7717 - val_loss: 0.9738 - val_acc: 0.7450\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8902 - acc: 0.7699 - val_loss: 0.9552 - val_acc: 0.7560\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8914 - acc: 0.7691 - val_loss: 0.9507 - val_acc: 0.7510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 473/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8905 - acc: 0.7692 - val_loss: 0.9614 - val_acc: 0.7530\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8902 - acc: 0.7709 - val_loss: 0.9496 - val_acc: 0.7550\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8880 - acc: 0.7717 - val_loss: 0.9519 - val_acc: 0.7580\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8892 - acc: 0.7713 - val_loss: 0.9746 - val_acc: 0.7490\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8892 - acc: 0.7699 - val_loss: 0.9498 - val_acc: 0.7520\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8881 - acc: 0.7725 - val_loss: 0.9522 - val_acc: 0.7540\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8886 - acc: 0.7699 - val_loss: 0.9506 - val_acc: 0.7470\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8873 - acc: 0.7731 - val_loss: 0.9522 - val_acc: 0.7500\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8865 - acc: 0.7727 - val_loss: 0.9521 - val_acc: 0.7490\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8868 - acc: 0.7713 - val_loss: 0.9506 - val_acc: 0.7450\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8863 - acc: 0.7709 - val_loss: 0.9499 - val_acc: 0.7480\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8864 - acc: 0.7733 - val_loss: 0.9604 - val_acc: 0.7490\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8865 - acc: 0.7695 - val_loss: 0.9467 - val_acc: 0.7520\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8858 - acc: 0.7705 - val_loss: 0.9496 - val_acc: 0.7540\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8858 - acc: 0.7713 - val_loss: 0.9589 - val_acc: 0.7470\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8847 - acc: 0.7704 - val_loss: 0.9588 - val_acc: 0.7490\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8858 - acc: 0.7717 - val_loss: 0.9509 - val_acc: 0.7460\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8844 - acc: 0.7716 - val_loss: 0.9516 - val_acc: 0.7500\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8846 - acc: 0.7737 - val_loss: 0.9499 - val_acc: 0.7530\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8836 - acc: 0.7725 - val_loss: 0.9482 - val_acc: 0.7550\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8853 - acc: 0.7699 - val_loss: 0.9520 - val_acc: 0.7490\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8827 - acc: 0.7732 - val_loss: 0.9516 - val_acc: 0.7530\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8830 - acc: 0.7716 - val_loss: 0.9526 - val_acc: 0.7490\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8833 - acc: 0.7724 - val_loss: 0.9511 - val_acc: 0.7580\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8822 - acc: 0.7720 - val_loss: 0.9444 - val_acc: 0.7530\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8817 - acc: 0.7733 - val_loss: 0.9716 - val_acc: 0.7460\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8832 - acc: 0.7727 - val_loss: 0.9466 - val_acc: 0.7500\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8814 - acc: 0.7719 - val_loss: 0.9477 - val_acc: 0.7510\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8809 - acc: 0.7725 - val_loss: 0.9442 - val_acc: 0.7530\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8805 - acc: 0.7713 - val_loss: 0.9467 - val_acc: 0.7510\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8805 - acc: 0.7733 - val_loss: 0.9439 - val_acc: 0.7530\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8799 - acc: 0.7744 - val_loss: 0.9530 - val_acc: 0.7480\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8809 - acc: 0.7704 - val_loss: 0.9471 - val_acc: 0.7520\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8803 - acc: 0.7720 - val_loss: 0.9456 - val_acc: 0.7560\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8798 - acc: 0.7732 - val_loss: 0.9441 - val_acc: 0.7480\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8789 - acc: 0.7721 - val_loss: 0.9583 - val_acc: 0.7490\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8803 - acc: 0.7720 - val_loss: 0.9460 - val_acc: 0.7470\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8786 - acc: 0.7732 - val_loss: 0.9603 - val_acc: 0.7500\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8784 - acc: 0.7741 - val_loss: 0.9694 - val_acc: 0.7480\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8795 - acc: 0.7713 - val_loss: 0.9570 - val_acc: 0.7490\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8783 - acc: 0.7737 - val_loss: 0.9452 - val_acc: 0.7470\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8774 - acc: 0.7729 - val_loss: 0.9471 - val_acc: 0.7470\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8772 - acc: 0.7713 - val_loss: 0.9527 - val_acc: 0.7470\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8780 - acc: 0.7729 - val_loss: 0.9424 - val_acc: 0.7480\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8773 - acc: 0.7745 - val_loss: 0.9516 - val_acc: 0.7460\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8774 - acc: 0.7731 - val_loss: 0.9392 - val_acc: 0.7480\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8758 - acc: 0.7743 - val_loss: 0.9413 - val_acc: 0.7550\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8764 - acc: 0.7739 - val_loss: 0.9650 - val_acc: 0.7470\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8767 - acc: 0.7737 - val_loss: 0.9414 - val_acc: 0.7520\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8758 - acc: 0.7740 - val_loss: 0.9454 - val_acc: 0.7520\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8752 - acc: 0.7743 - val_loss: 0.9401 - val_acc: 0.7510\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8759 - acc: 0.7741 - val_loss: 0.9513 - val_acc: 0.7510\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8771 - acc: 0.7723 - val_loss: 0.9421 - val_acc: 0.7520\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8759 - acc: 0.7724 - val_loss: 0.9533 - val_acc: 0.7520\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8746 - acc: 0.7719 - val_loss: 0.9686 - val_acc: 0.7440\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8757 - acc: 0.7747 - val_loss: 0.9440 - val_acc: 0.7580\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8743 - acc: 0.7739 - val_loss: 0.9431 - val_acc: 0.7550\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8735 - acc: 0.7732 - val_loss: 0.9421 - val_acc: 0.7490\n",
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8736 - acc: 0.7743 - val_loss: 0.9432 - val_acc: 0.7560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 532/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8725 - acc: 0.7749 - val_loss: 0.9421 - val_acc: 0.7500\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8726 - acc: 0.7748 - val_loss: 0.9399 - val_acc: 0.7520\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8736 - acc: 0.7729 - val_loss: 0.9421 - val_acc: 0.7500\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8721 - acc: 0.7751 - val_loss: 0.9391 - val_acc: 0.7580\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8725 - acc: 0.7753 - val_loss: 0.9568 - val_acc: 0.7470\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8717 - acc: 0.7740 - val_loss: 0.9378 - val_acc: 0.7520\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8729 - acc: 0.7736 - val_loss: 0.9412 - val_acc: 0.7530\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8715 - acc: 0.7749 - val_loss: 0.9400 - val_acc: 0.7560\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8708 - acc: 0.7740 - val_loss: 0.9497 - val_acc: 0.7460\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8716 - acc: 0.7725 - val_loss: 0.9496 - val_acc: 0.7500\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8709 - acc: 0.7748 - val_loss: 0.9377 - val_acc: 0.7540\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8698 - acc: 0.7744 - val_loss: 0.9431 - val_acc: 0.7560\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8698 - acc: 0.7756 - val_loss: 0.9473 - val_acc: 0.7510\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8707 - acc: 0.7739 - val_loss: 0.9429 - val_acc: 0.7540\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8690 - acc: 0.7760 - val_loss: 0.9474 - val_acc: 0.7540\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8694 - acc: 0.7751 - val_loss: 0.9505 - val_acc: 0.7480\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8696 - acc: 0.7737 - val_loss: 0.9394 - val_acc: 0.7520\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8692 - acc: 0.7745 - val_loss: 0.9412 - val_acc: 0.7490\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8698 - acc: 0.7735 - val_loss: 0.9495 - val_acc: 0.7520\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8688 - acc: 0.7740 - val_loss: 0.9456 - val_acc: 0.7490\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8694 - acc: 0.7752 - val_loss: 0.9425 - val_acc: 0.7510\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8692 - acc: 0.7741 - val_loss: 0.9419 - val_acc: 0.7540\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8687 - acc: 0.7727 - val_loss: 0.9392 - val_acc: 0.7520\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8683 - acc: 0.7755 - val_loss: 0.9432 - val_acc: 0.7480\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8682 - acc: 0.7739 - val_loss: 0.9430 - val_acc: 0.7490\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8694 - acc: 0.7743 - val_loss: 0.9354 - val_acc: 0.7540\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8685 - acc: 0.7737 - val_loss: 0.9414 - val_acc: 0.7520\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8685 - acc: 0.7740 - val_loss: 0.9426 - val_acc: 0.7520\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8670 - acc: 0.7743 - val_loss: 0.9401 - val_acc: 0.7490\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8665 - acc: 0.7759 - val_loss: 0.9513 - val_acc: 0.7470\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8664 - acc: 0.7756 - val_loss: 0.9396 - val_acc: 0.7530\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8664 - acc: 0.7752 - val_loss: 0.9410 - val_acc: 0.7510\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8660 - acc: 0.7755 - val_loss: 0.9530 - val_acc: 0.7430\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8675 - acc: 0.7737 - val_loss: 0.9531 - val_acc: 0.7520\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8669 - acc: 0.7747 - val_loss: 0.9421 - val_acc: 0.7520\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8660 - acc: 0.7739 - val_loss: 0.9346 - val_acc: 0.7560\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8640 - acc: 0.7773 - val_loss: 0.9393 - val_acc: 0.7460\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8663 - acc: 0.7757 - val_loss: 0.9391 - val_acc: 0.7520\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8662 - acc: 0.7753 - val_loss: 0.9629 - val_acc: 0.7470\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8658 - acc: 0.7745 - val_loss: 0.9421 - val_acc: 0.7470\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8643 - acc: 0.7756 - val_loss: 0.9430 - val_acc: 0.7560\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8652 - acc: 0.7744 - val_loss: 0.9366 - val_acc: 0.7470\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8637 - acc: 0.7755 - val_loss: 0.9429 - val_acc: 0.7450\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8635 - acc: 0.7748 - val_loss: 0.9361 - val_acc: 0.7490\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8638 - acc: 0.7759 - val_loss: 0.9443 - val_acc: 0.7450\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8636 - acc: 0.7752 - val_loss: 0.9405 - val_acc: 0.7490\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8641 - acc: 0.7756 - val_loss: 0.9379 - val_acc: 0.7570\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8646 - acc: 0.7743 - val_loss: 0.9367 - val_acc: 0.7490\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8634 - acc: 0.7753 - val_loss: 0.9392 - val_acc: 0.7480\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8617 - acc: 0.7760 - val_loss: 0.9451 - val_acc: 0.7440\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8620 - acc: 0.7771 - val_loss: 0.9334 - val_acc: 0.7540\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8625 - acc: 0.7757 - val_loss: 0.9384 - val_acc: 0.7520\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8630 - acc: 0.7739 - val_loss: 0.9523 - val_acc: 0.7490\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8640 - acc: 0.7748 - val_loss: 0.9357 - val_acc: 0.7550\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8613 - acc: 0.7747 - val_loss: 0.9389 - val_acc: 0.7480\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8614 - acc: 0.7759 - val_loss: 0.9602 - val_acc: 0.7440\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8618 - acc: 0.7761 - val_loss: 0.9357 - val_acc: 0.7510\n",
      "Epoch 589/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8613 - acc: 0.7757 - val_loss: 0.9353 - val_acc: 0.7520\n",
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8620 - acc: 0.7761 - val_loss: 0.9371 - val_acc: 0.7530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 591/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8603 - acc: 0.7768 - val_loss: 0.9372 - val_acc: 0.7520\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8602 - acc: 0.7763 - val_loss: 0.9383 - val_acc: 0.7490\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8596 - acc: 0.7776 - val_loss: 0.9341 - val_acc: 0.7490\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8601 - acc: 0.7769 - val_loss: 0.9485 - val_acc: 0.7550\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8598 - acc: 0.7775 - val_loss: 0.9351 - val_acc: 0.7520\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8591 - acc: 0.7775 - val_loss: 0.9326 - val_acc: 0.7510\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8593 - acc: 0.7781 - val_loss: 0.9410 - val_acc: 0.7520\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8593 - acc: 0.7743 - val_loss: 0.9319 - val_acc: 0.7520\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8619 - acc: 0.7757 - val_loss: 0.9474 - val_acc: 0.7570\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8595 - acc: 0.7768 - val_loss: 0.9433 - val_acc: 0.7470\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8597 - acc: 0.7749 - val_loss: 0.9310 - val_acc: 0.7550\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8581 - acc: 0.7771 - val_loss: 0.9334 - val_acc: 0.7500\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8579 - acc: 0.7775 - val_loss: 0.9523 - val_acc: 0.7480\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8578 - acc: 0.7751 - val_loss: 0.9391 - val_acc: 0.7540\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8573 - acc: 0.7773 - val_loss: 0.9457 - val_acc: 0.7510\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8578 - acc: 0.7775 - val_loss: 0.9362 - val_acc: 0.7550\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8596 - acc: 0.7761 - val_loss: 0.9303 - val_acc: 0.7540\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8567 - acc: 0.7777 - val_loss: 0.9372 - val_acc: 0.7470\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8582 - acc: 0.7744 - val_loss: 0.9353 - val_acc: 0.7600\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8582 - acc: 0.7777 - val_loss: 0.9311 - val_acc: 0.7530\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8577 - acc: 0.7759 - val_loss: 0.9411 - val_acc: 0.7450\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8560 - acc: 0.7775 - val_loss: 0.9410 - val_acc: 0.7500\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8579 - acc: 0.7764 - val_loss: 0.9464 - val_acc: 0.7490\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8582 - acc: 0.7779 - val_loss: 0.9400 - val_acc: 0.7480\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8559 - acc: 0.7783 - val_loss: 0.9380 - val_acc: 0.7540\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8557 - acc: 0.7755 - val_loss: 0.9315 - val_acc: 0.7560\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8558 - acc: 0.7773 - val_loss: 0.9369 - val_acc: 0.7490\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8568 - acc: 0.7783 - val_loss: 0.9428 - val_acc: 0.7470\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8558 - acc: 0.7805 - val_loss: 0.9598 - val_acc: 0.7520\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8580 - acc: 0.7752 - val_loss: 0.9328 - val_acc: 0.7550\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8554 - acc: 0.7785 - val_loss: 0.9513 - val_acc: 0.7550\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8565 - acc: 0.7761 - val_loss: 0.9462 - val_acc: 0.7450\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8552 - acc: 0.7781 - val_loss: 0.9305 - val_acc: 0.7530\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8549 - acc: 0.7771 - val_loss: 0.9319 - val_acc: 0.7520\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8547 - acc: 0.7793 - val_loss: 0.9524 - val_acc: 0.7470\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8560 - acc: 0.7772 - val_loss: 0.9469 - val_acc: 0.7500\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8565 - acc: 0.7788 - val_loss: 0.9306 - val_acc: 0.7480\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8540 - acc: 0.7779 - val_loss: 0.9307 - val_acc: 0.7570\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8542 - acc: 0.7761 - val_loss: 0.9418 - val_acc: 0.7530\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8574 - acc: 0.7747 - val_loss: 0.9404 - val_acc: 0.7520\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8536 - acc: 0.7767 - val_loss: 0.9339 - val_acc: 0.7490\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8521 - acc: 0.7805 - val_loss: 0.9287 - val_acc: 0.7500\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8527 - acc: 0.7788 - val_loss: 0.9342 - val_acc: 0.7480\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8527 - acc: 0.7795 - val_loss: 0.9368 - val_acc: 0.7480\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8526 - acc: 0.7795 - val_loss: 0.9444 - val_acc: 0.7480\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8523 - acc: 0.7776 - val_loss: 0.9365 - val_acc: 0.7430\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8527 - acc: 0.7783 - val_loss: 0.9399 - val_acc: 0.7540\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8532 - acc: 0.7799 - val_loss: 0.9399 - val_acc: 0.7410\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8523 - acc: 0.7788 - val_loss: 0.9277 - val_acc: 0.7490\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8512 - acc: 0.7784 - val_loss: 0.9305 - val_acc: 0.7560\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8525 - acc: 0.7775 - val_loss: 0.9334 - val_acc: 0.7510\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8521 - acc: 0.7764 - val_loss: 0.9304 - val_acc: 0.7510\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8504 - acc: 0.7779 - val_loss: 0.9323 - val_acc: 0.7430\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8513 - acc: 0.7793 - val_loss: 0.9371 - val_acc: 0.7510\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8504 - acc: 0.7792 - val_loss: 0.9321 - val_acc: 0.7500\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8512 - acc: 0.7791 - val_loss: 0.9347 - val_acc: 0.7520\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8527 - acc: 0.7768 - val_loss: 0.9408 - val_acc: 0.7430\n",
      "Epoch 648/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8515 - acc: 0.7785 - val_loss: 0.9323 - val_acc: 0.7490\n",
      "Epoch 649/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8519 - acc: 0.7781 - val_loss: 0.9334 - val_acc: 0.7510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 650/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8529 - acc: 0.7765 - val_loss: 0.9351 - val_acc: 0.7560\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8504 - acc: 0.7780 - val_loss: 0.9475 - val_acc: 0.7500\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8515 - acc: 0.7759 - val_loss: 0.9330 - val_acc: 0.7480\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8506 - acc: 0.7784 - val_loss: 0.9389 - val_acc: 0.7510\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8495 - acc: 0.7809 - val_loss: 0.9502 - val_acc: 0.7460\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8515 - acc: 0.7788 - val_loss: 0.9332 - val_acc: 0.7580\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8499 - acc: 0.7784 - val_loss: 0.9351 - val_acc: 0.7590\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8507 - acc: 0.7801 - val_loss: 0.9501 - val_acc: 0.7510\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8496 - acc: 0.7779 - val_loss: 0.9268 - val_acc: 0.7490\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8507 - acc: 0.7781 - val_loss: 0.9563 - val_acc: 0.7530\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8510 - acc: 0.7791 - val_loss: 0.9380 - val_acc: 0.7550\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8497 - acc: 0.7799 - val_loss: 0.9356 - val_acc: 0.7550\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8478 - acc: 0.7800 - val_loss: 0.9306 - val_acc: 0.7510\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8475 - acc: 0.7811 - val_loss: 0.9847 - val_acc: 0.7440\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8502 - acc: 0.7796 - val_loss: 0.9327 - val_acc: 0.7570\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8486 - acc: 0.7777 - val_loss: 0.9314 - val_acc: 0.7520\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8481 - acc: 0.7811 - val_loss: 0.9277 - val_acc: 0.7540\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8488 - acc: 0.7796 - val_loss: 0.9384 - val_acc: 0.7430\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8489 - acc: 0.7803 - val_loss: 0.9287 - val_acc: 0.7550\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8480 - acc: 0.7787 - val_loss: 0.9383 - val_acc: 0.7480\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8478 - acc: 0.7819 - val_loss: 0.9302 - val_acc: 0.7550\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8473 - acc: 0.7775 - val_loss: 0.9431 - val_acc: 0.7500\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8484 - acc: 0.7783 - val_loss: 0.9318 - val_acc: 0.7540\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8494 - acc: 0.7800 - val_loss: 0.9284 - val_acc: 0.7540\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8476 - acc: 0.7803 - val_loss: 0.9369 - val_acc: 0.7540\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8479 - acc: 0.7793 - val_loss: 0.9514 - val_acc: 0.7460\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8467 - acc: 0.7795 - val_loss: 0.9274 - val_acc: 0.7480\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8468 - acc: 0.7803 - val_loss: 0.9498 - val_acc: 0.7460\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8463 - acc: 0.7809 - val_loss: 0.9256 - val_acc: 0.7510\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8445 - acc: 0.7831 - val_loss: 0.9386 - val_acc: 0.7490\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8464 - acc: 0.7813 - val_loss: 0.9371 - val_acc: 0.7470\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8481 - acc: 0.7809 - val_loss: 0.9290 - val_acc: 0.7530\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8455 - acc: 0.7803 - val_loss: 0.9398 - val_acc: 0.7440\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8459 - acc: 0.7783 - val_loss: 0.9310 - val_acc: 0.7570\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8458 - acc: 0.7801 - val_loss: 0.9270 - val_acc: 0.7510\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8440 - acc: 0.7817 - val_loss: 0.9326 - val_acc: 0.7490\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8486 - acc: 0.7777 - val_loss: 0.9308 - val_acc: 0.7490\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8466 - acc: 0.7831 - val_loss: 0.9663 - val_acc: 0.7370\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8467 - acc: 0.7817 - val_loss: 0.9341 - val_acc: 0.7500\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8460 - acc: 0.7803 - val_loss: 0.9295 - val_acc: 0.7510\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8457 - acc: 0.7799 - val_loss: 0.9310 - val_acc: 0.7520\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8453 - acc: 0.7800 - val_loss: 0.9300 - val_acc: 0.7490\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8425 - acc: 0.7820 - val_loss: 0.9330 - val_acc: 0.7540\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8441 - acc: 0.7819 - val_loss: 0.9242 - val_acc: 0.7510\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8445 - acc: 0.7801 - val_loss: 0.9283 - val_acc: 0.7480\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8432 - acc: 0.7815 - val_loss: 0.9354 - val_acc: 0.7530\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8451 - acc: 0.7803 - val_loss: 0.9430 - val_acc: 0.7530\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8439 - acc: 0.7843 - val_loss: 0.9236 - val_acc: 0.7550\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8417 - acc: 0.7812 - val_loss: 0.9297 - val_acc: 0.7450\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8425 - acc: 0.7805 - val_loss: 0.9271 - val_acc: 0.7530\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8424 - acc: 0.7820 - val_loss: 0.9370 - val_acc: 0.7530\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8441 - acc: 0.7812 - val_loss: 0.9368 - val_acc: 0.7510\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8438 - acc: 0.7824 - val_loss: 0.9287 - val_acc: 0.7470\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8432 - acc: 0.7792 - val_loss: 0.9306 - val_acc: 0.7500\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8419 - acc: 0.7803 - val_loss: 0.9258 - val_acc: 0.7530\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8414 - acc: 0.7824 - val_loss: 0.9230 - val_acc: 0.7550\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8411 - acc: 0.7813 - val_loss: 0.9286 - val_acc: 0.7550\n",
      "Epoch 707/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8418 - acc: 0.7809 - val_loss: 0.9577 - val_acc: 0.7390\n",
      "Epoch 708/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8435 - acc: 0.7804 - val_loss: 0.9272 - val_acc: 0.7530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 709/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8424 - acc: 0.7796 - val_loss: 0.9290 - val_acc: 0.7510\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8420 - acc: 0.7812 - val_loss: 0.9506 - val_acc: 0.7500\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8415 - acc: 0.7821 - val_loss: 0.9267 - val_acc: 0.7520\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8428 - acc: 0.7819 - val_loss: 0.9557 - val_acc: 0.7460\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8425 - acc: 0.7797 - val_loss: 0.9307 - val_acc: 0.7550\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8405 - acc: 0.7811 - val_loss: 0.9332 - val_acc: 0.7450\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8422 - acc: 0.7807 - val_loss: 0.9594 - val_acc: 0.7340\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8417 - acc: 0.7825 - val_loss: 0.9281 - val_acc: 0.7550\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8409 - acc: 0.7817 - val_loss: 0.9262 - val_acc: 0.7560\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8391 - acc: 0.7811 - val_loss: 0.9581 - val_acc: 0.7350\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8424 - acc: 0.7815 - val_loss: 0.9273 - val_acc: 0.7530\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8404 - acc: 0.7811 - val_loss: 1.0014 - val_acc: 0.7390\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8415 - acc: 0.7809 - val_loss: 0.9290 - val_acc: 0.7460\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8404 - acc: 0.7829 - val_loss: 0.9250 - val_acc: 0.7560\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8418 - acc: 0.7820 - val_loss: 0.9265 - val_acc: 0.7520\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.8366 - acc: 0.779 - 0s 32us/step - loss: 0.8397 - acc: 0.7815 - val_loss: 0.9427 - val_acc: 0.7410\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8400 - acc: 0.7819 - val_loss: 0.9401 - val_acc: 0.7470\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8414 - acc: 0.7837 - val_loss: 0.9268 - val_acc: 0.7450\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8386 - acc: 0.7844 - val_loss: 0.9230 - val_acc: 0.7490\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8371 - acc: 0.7840 - val_loss: 0.9421 - val_acc: 0.7380\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8398 - acc: 0.7819 - val_loss: 0.9225 - val_acc: 0.7510\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8391 - acc: 0.7831 - val_loss: 0.9290 - val_acc: 0.7500\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8390 - acc: 0.7813 - val_loss: 0.9252 - val_acc: 0.7490\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8386 - acc: 0.7851 - val_loss: 0.9444 - val_acc: 0.7430\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8394 - acc: 0.7833 - val_loss: 0.9255 - val_acc: 0.7560\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8385 - acc: 0.7837 - val_loss: 0.9469 - val_acc: 0.7430\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8368 - acc: 0.7825 - val_loss: 0.9385 - val_acc: 0.7510\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8366 - acc: 0.7848 - val_loss: 0.9254 - val_acc: 0.7490\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8390 - acc: 0.7824 - val_loss: 0.9282 - val_acc: 0.7570\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8361 - acc: 0.7845 - val_loss: 0.9412 - val_acc: 0.7500\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8381 - acc: 0.7843 - val_loss: 0.9311 - val_acc: 0.7560\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8383 - acc: 0.7833 - val_loss: 0.9223 - val_acc: 0.7540\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8377 - acc: 0.7828 - val_loss: 0.9240 - val_acc: 0.7520\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8361 - acc: 0.7844 - val_loss: 0.9484 - val_acc: 0.7510\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8379 - acc: 0.7829 - val_loss: 0.9284 - val_acc: 0.7500\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8360 - acc: 0.7860 - val_loss: 0.9236 - val_acc: 0.7520\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8359 - acc: 0.7817 - val_loss: 0.9395 - val_acc: 0.7460\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8363 - acc: 0.7845 - val_loss: 0.9216 - val_acc: 0.7530\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8352 - acc: 0.7849 - val_loss: 0.9869 - val_acc: 0.7310\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8437 - acc: 0.7791 - val_loss: 0.9310 - val_acc: 0.7520\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8367 - acc: 0.7839 - val_loss: 0.9223 - val_acc: 0.7530\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8356 - acc: 0.7849 - val_loss: 0.9293 - val_acc: 0.7430\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8350 - acc: 0.7864 - val_loss: 0.9250 - val_acc: 0.7530\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8367 - acc: 0.7833 - val_loss: 0.9471 - val_acc: 0.7390\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8377 - acc: 0.7833 - val_loss: 0.9288 - val_acc: 0.7480\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8348 - acc: 0.7873 - val_loss: 0.9257 - val_acc: 0.7530\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8348 - acc: 0.7853 - val_loss: 0.9453 - val_acc: 0.7520\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8388 - acc: 0.7832 - val_loss: 0.9230 - val_acc: 0.7540\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8348 - acc: 0.7825 - val_loss: 0.9230 - val_acc: 0.7540\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8357 - acc: 0.7812 - val_loss: 0.9627 - val_acc: 0.7370\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8366 - acc: 0.7819 - val_loss: 0.9279 - val_acc: 0.7470\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8348 - acc: 0.7853 - val_loss: 0.9346 - val_acc: 0.7550\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8333 - acc: 0.7836 - val_loss: 0.9272 - val_acc: 0.7610\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8346 - acc: 0.7840 - val_loss: 0.9297 - val_acc: 0.7470\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8336 - acc: 0.7852 - val_loss: 0.9339 - val_acc: 0.7540\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8343 - acc: 0.7816 - val_loss: 0.9379 - val_acc: 0.7550\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8348 - acc: 0.7839 - val_loss: 0.9188 - val_acc: 0.7500\n",
      "Epoch 766/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8341 - acc: 0.7853 - val_loss: 0.9287 - val_acc: 0.7460\n",
      "Epoch 767/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8324 - acc: 0.7871 - val_loss: 0.9267 - val_acc: 0.7520\n",
      "Epoch 768/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8315 - acc: 0.7887 - val_loss: 0.9230 - val_acc: 0.7510\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8322 - acc: 0.7859 - val_loss: 0.9723 - val_acc: 0.7440\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8351 - acc: 0.7847 - val_loss: 0.9232 - val_acc: 0.7520\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8320 - acc: 0.7844 - val_loss: 0.9219 - val_acc: 0.7500\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8328 - acc: 0.7857 - val_loss: 0.9200 - val_acc: 0.7530\n",
      "Epoch 773/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8333 - acc: 0.7868 - val_loss: 0.9276 - val_acc: 0.7550\n",
      "Epoch 774/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8317 - acc: 0.7860 - val_loss: 0.9299 - val_acc: 0.7500\n",
      "Epoch 775/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8321 - acc: 0.7849 - val_loss: 0.9244 - val_acc: 0.7530\n",
      "Epoch 776/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8326 - acc: 0.7859 - val_loss: 0.9233 - val_acc: 0.7490\n",
      "Epoch 777/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8319 - acc: 0.7853 - val_loss: 0.9322 - val_acc: 0.7550\n",
      "Epoch 778/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8328 - acc: 0.7869 - val_loss: 0.9343 - val_acc: 0.7510\n",
      "Epoch 779/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8330 - acc: 0.7851 - val_loss: 0.9487 - val_acc: 0.7500\n",
      "Epoch 780/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8343 - acc: 0.7864 - val_loss: 0.9598 - val_acc: 0.7400\n",
      "Epoch 781/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8325 - acc: 0.7857 - val_loss: 0.9448 - val_acc: 0.7550\n",
      "Epoch 782/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8321 - acc: 0.7819 - val_loss: 0.9387 - val_acc: 0.7460\n",
      "Epoch 783/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8331 - acc: 0.7848 - val_loss: 0.9214 - val_acc: 0.7480\n",
      "Epoch 784/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8305 - acc: 0.7848 - val_loss: 0.9427 - val_acc: 0.7500\n",
      "Epoch 785/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8333 - acc: 0.7843 - val_loss: 0.9180 - val_acc: 0.7560\n",
      "Epoch 786/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8323 - acc: 0.7844 - val_loss: 0.9256 - val_acc: 0.7540\n",
      "Epoch 787/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8321 - acc: 0.7845 - val_loss: 0.9472 - val_acc: 0.7460\n",
      "Epoch 788/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8305 - acc: 0.7839 - val_loss: 0.9165 - val_acc: 0.7530\n",
      "Epoch 789/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8299 - acc: 0.7871 - val_loss: 0.9338 - val_acc: 0.7520\n",
      "Epoch 790/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8282 - acc: 0.7864 - val_loss: 0.9184 - val_acc: 0.7520\n",
      "Epoch 791/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8283 - acc: 0.7869 - val_loss: 0.9180 - val_acc: 0.7540\n",
      "Epoch 792/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8305 - acc: 0.7849 - val_loss: 0.9350 - val_acc: 0.7560\n",
      "Epoch 793/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8307 - acc: 0.7877 - val_loss: 0.9342 - val_acc: 0.7460\n",
      "Epoch 794/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8275 - acc: 0.7880 - val_loss: 0.9227 - val_acc: 0.7450\n",
      "Epoch 795/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8288 - acc: 0.7864 - val_loss: 0.9164 - val_acc: 0.7540\n",
      "Epoch 796/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8290 - acc: 0.7871 - val_loss: 0.9337 - val_acc: 0.7460\n",
      "Epoch 797/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8359 - acc: 0.7836 - val_loss: 0.9319 - val_acc: 0.7500\n",
      "Epoch 798/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8295 - acc: 0.7857 - val_loss: 0.9253 - val_acc: 0.7510\n",
      "Epoch 799/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8302 - acc: 0.7864 - val_loss: 0.9414 - val_acc: 0.7510\n",
      "Epoch 800/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8280 - acc: 0.7863 - val_loss: 0.9272 - val_acc: 0.7570\n",
      "Epoch 801/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8304 - acc: 0.7871 - val_loss: 0.9219 - val_acc: 0.7480\n",
      "Epoch 802/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8279 - acc: 0.7853 - val_loss: 0.9242 - val_acc: 0.7500\n",
      "Epoch 803/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8278 - acc: 0.7868 - val_loss: 0.9420 - val_acc: 0.7470\n",
      "Epoch 804/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8284 - acc: 0.7868 - val_loss: 0.9330 - val_acc: 0.7470\n",
      "Epoch 805/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8323 - acc: 0.7853 - val_loss: 0.9435 - val_acc: 0.7410\n",
      "Epoch 806/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8284 - acc: 0.7843 - val_loss: 0.9439 - val_acc: 0.7430\n",
      "Epoch 807/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8308 - acc: 0.7847 - val_loss: 0.9166 - val_acc: 0.7550\n",
      "Epoch 808/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8264 - acc: 0.7908 - val_loss: 0.9500 - val_acc: 0.7510\n",
      "Epoch 809/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8290 - acc: 0.7883 - val_loss: 0.9189 - val_acc: 0.7530\n",
      "Epoch 810/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8287 - acc: 0.7868 - val_loss: 0.9281 - val_acc: 0.7480\n",
      "Epoch 811/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8294 - acc: 0.7869 - val_loss: 0.9287 - val_acc: 0.7510\n",
      "Epoch 812/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8288 - acc: 0.7875 - val_loss: 0.9405 - val_acc: 0.7480\n",
      "Epoch 813/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8293 - acc: 0.7848 - val_loss: 0.9241 - val_acc: 0.7550\n",
      "Epoch 814/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8285 - acc: 0.7840 - val_loss: 0.9358 - val_acc: 0.7450\n",
      "Epoch 815/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8280 - acc: 0.7875 - val_loss: 0.9249 - val_acc: 0.7500\n",
      "Epoch 816/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8276 - acc: 0.7868 - val_loss: 0.9355 - val_acc: 0.7500\n",
      "Epoch 817/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8249 - acc: 0.7884 - val_loss: 0.9531 - val_acc: 0.7420\n",
      "Epoch 818/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8280 - acc: 0.7889 - val_loss: 0.9385 - val_acc: 0.7440\n",
      "Epoch 819/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8284 - acc: 0.7877 - val_loss: 0.9289 - val_acc: 0.7490\n",
      "Epoch 820/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8242 - acc: 0.7888 - val_loss: 0.9161 - val_acc: 0.7560\n",
      "Epoch 821/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8266 - acc: 0.7885 - val_loss: 0.9217 - val_acc: 0.7510\n",
      "Epoch 822/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8257 - acc: 0.7828 - val_loss: 0.9470 - val_acc: 0.7550\n",
      "Epoch 823/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8256 - acc: 0.7904 - val_loss: 0.9317 - val_acc: 0.7560\n",
      "Epoch 824/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8260 - acc: 0.7893 - val_loss: 0.9233 - val_acc: 0.7500\n",
      "Epoch 825/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8267 - acc: 0.7876 - val_loss: 0.9423 - val_acc: 0.7510\n",
      "Epoch 826/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8277 - acc: 0.7863 - val_loss: 0.9316 - val_acc: 0.7380\n",
      "Epoch 827/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8269 - acc: 0.7853 - val_loss: 0.9336 - val_acc: 0.7560\n",
      "Epoch 828/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8238 - acc: 0.7881 - val_loss: 0.9535 - val_acc: 0.7460\n",
      "Epoch 829/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8260 - acc: 0.7884 - val_loss: 0.9225 - val_acc: 0.7530\n",
      "Epoch 830/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8243 - acc: 0.7888 - val_loss: 0.9972 - val_acc: 0.7400\n",
      "Epoch 831/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8264 - acc: 0.7863 - val_loss: 0.9196 - val_acc: 0.7520\n",
      "Epoch 832/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8261 - acc: 0.7876 - val_loss: 0.9220 - val_acc: 0.7530\n",
      "Epoch 833/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8275 - acc: 0.7865 - val_loss: 0.9305 - val_acc: 0.7500\n",
      "Epoch 834/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8255 - acc: 0.7881 - val_loss: 0.9318 - val_acc: 0.7550\n",
      "Epoch 835/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8251 - acc: 0.7867 - val_loss: 0.9206 - val_acc: 0.7520\n",
      "Epoch 836/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8252 - acc: 0.7873 - val_loss: 0.9232 - val_acc: 0.7530\n",
      "Epoch 837/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8250 - acc: 0.7857 - val_loss: 0.9224 - val_acc: 0.7480\n",
      "Epoch 838/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8260 - acc: 0.7885 - val_loss: 0.9293 - val_acc: 0.7490\n",
      "Epoch 839/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8231 - acc: 0.7888 - val_loss: 0.9577 - val_acc: 0.7560\n",
      "Epoch 840/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8286 - acc: 0.7849 - val_loss: 0.9184 - val_acc: 0.7510\n",
      "Epoch 841/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8234 - acc: 0.7893 - val_loss: 0.9510 - val_acc: 0.7520\n",
      "Epoch 842/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8236 - acc: 0.7900 - val_loss: 0.9249 - val_acc: 0.7490\n",
      "Epoch 843/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8237 - acc: 0.7889 - val_loss: 0.9225 - val_acc: 0.7490\n",
      "Epoch 844/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8233 - acc: 0.7869 - val_loss: 0.9206 - val_acc: 0.7540\n",
      "Epoch 845/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8214 - acc: 0.7891 - val_loss: 0.9200 - val_acc: 0.7540\n",
      "Epoch 846/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8208 - acc: 0.7869 - val_loss: 0.9134 - val_acc: 0.7540\n",
      "Epoch 847/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8216 - acc: 0.7889 - val_loss: 0.9393 - val_acc: 0.7480\n",
      "Epoch 848/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8211 - acc: 0.7893 - val_loss: 0.9424 - val_acc: 0.7520\n",
      "Epoch 849/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8221 - acc: 0.7883 - val_loss: 0.9588 - val_acc: 0.7480\n",
      "Epoch 850/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8228 - acc: 0.7880 - val_loss: 0.9383 - val_acc: 0.7430\n",
      "Epoch 851/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8231 - acc: 0.7896 - val_loss: 0.9198 - val_acc: 0.7550\n",
      "Epoch 852/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8222 - acc: 0.7887 - val_loss: 0.9173 - val_acc: 0.7540\n",
      "Epoch 853/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8221 - acc: 0.7881 - val_loss: 0.9207 - val_acc: 0.7590\n",
      "Epoch 854/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8215 - acc: 0.7905 - val_loss: 0.9241 - val_acc: 0.7520\n",
      "Epoch 855/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8207 - acc: 0.7899 - val_loss: 0.9525 - val_acc: 0.7500\n",
      "Epoch 856/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8213 - acc: 0.7889 - val_loss: 0.9439 - val_acc: 0.7500\n",
      "Epoch 857/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8227 - acc: 0.7909 - val_loss: 0.9178 - val_acc: 0.7570\n",
      "Epoch 858/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8193 - acc: 0.7920 - val_loss: 0.9159 - val_acc: 0.7480\n",
      "Epoch 859/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8222 - acc: 0.7873 - val_loss: 0.9395 - val_acc: 0.7490\n",
      "Epoch 860/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8249 - acc: 0.7868 - val_loss: 0.9393 - val_acc: 0.7520\n",
      "Epoch 861/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8206 - acc: 0.7895 - val_loss: 0.9137 - val_acc: 0.7540\n",
      "Epoch 862/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8244 - acc: 0.7868 - val_loss: 0.9157 - val_acc: 0.7500\n",
      "Epoch 863/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8209 - acc: 0.7892 - val_loss: 0.9274 - val_acc: 0.7450\n",
      "Epoch 864/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8188 - acc: 0.7889 - val_loss: 0.9322 - val_acc: 0.7500\n",
      "Epoch 865/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8218 - acc: 0.7881 - val_loss: 0.9121 - val_acc: 0.7560\n",
      "Epoch 866/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8176 - acc: 0.7924 - val_loss: 0.9142 - val_acc: 0.7510\n",
      "Epoch 867/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8202 - acc: 0.7863 - val_loss: 0.9262 - val_acc: 0.7510\n",
      "Epoch 868/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8208 - acc: 0.7908 - val_loss: 0.9277 - val_acc: 0.7560\n",
      "Epoch 869/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8199 - acc: 0.7859 - val_loss: 0.9318 - val_acc: 0.7510\n",
      "Epoch 870/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8221 - acc: 0.7879 - val_loss: 0.9172 - val_acc: 0.7580\n",
      "Epoch 871/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8196 - acc: 0.7893 - val_loss: 0.9260 - val_acc: 0.7470\n",
      "Epoch 872/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8193 - acc: 0.7912 - val_loss: 0.9302 - val_acc: 0.7510\n",
      "Epoch 873/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8267 - acc: 0.7877 - val_loss: 0.9137 - val_acc: 0.7470\n",
      "Epoch 874/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8198 - acc: 0.7907 - val_loss: 0.9333 - val_acc: 0.7420\n",
      "Epoch 875/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8189 - acc: 0.7884 - val_loss: 0.9140 - val_acc: 0.7520\n",
      "Epoch 876/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8225 - acc: 0.7867 - val_loss: 0.9284 - val_acc: 0.7540\n",
      "Epoch 877/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8181 - acc: 0.7909 - val_loss: 0.9162 - val_acc: 0.7470\n",
      "Epoch 878/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8188 - acc: 0.7892 - val_loss: 0.9253 - val_acc: 0.7410\n",
      "Epoch 879/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8220 - acc: 0.7891 - val_loss: 0.9201 - val_acc: 0.7540\n",
      "Epoch 880/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8190 - acc: 0.7899 - val_loss: 0.9207 - val_acc: 0.7550\n",
      "Epoch 881/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8206 - acc: 0.7892 - val_loss: 0.9413 - val_acc: 0.7370\n",
      "Epoch 882/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8204 - acc: 0.7921 - val_loss: 0.9302 - val_acc: 0.7520\n",
      "Epoch 883/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8153 - acc: 0.7916 - val_loss: 0.9613 - val_acc: 0.7440\n",
      "Epoch 884/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8221 - acc: 0.7895 - val_loss: 0.9129 - val_acc: 0.7520\n",
      "Epoch 885/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8177 - acc: 0.7903 - val_loss: 0.9202 - val_acc: 0.7530\n",
      "Epoch 886/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8179 - acc: 0.7925 - val_loss: 0.9455 - val_acc: 0.7530\n",
      "Epoch 887/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8192 - acc: 0.7891 - val_loss: 0.9154 - val_acc: 0.7570\n",
      "Epoch 888/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8183 - acc: 0.7863 - val_loss: 0.9218 - val_acc: 0.7580\n",
      "Epoch 889/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8152 - acc: 0.7928 - val_loss: 0.9415 - val_acc: 0.7400\n",
      "Epoch 890/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8217 - acc: 0.7879 - val_loss: 0.9343 - val_acc: 0.7530\n",
      "Epoch 891/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8156 - acc: 0.7925 - val_loss: 0.9336 - val_acc: 0.7470\n",
      "Epoch 892/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8222 - acc: 0.7887 - val_loss: 0.9203 - val_acc: 0.7510\n",
      "Epoch 893/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8182 - acc: 0.7893 - val_loss: 0.9196 - val_acc: 0.7520\n",
      "Epoch 894/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8194 - acc: 0.7909 - val_loss: 0.9157 - val_acc: 0.7550\n",
      "Epoch 895/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8175 - acc: 0.7909 - val_loss: 0.9310 - val_acc: 0.7480\n",
      "Epoch 896/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8196 - acc: 0.7892 - val_loss: 0.9262 - val_acc: 0.7500\n",
      "Epoch 897/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8155 - acc: 0.7900 - val_loss: 0.9140 - val_acc: 0.7480\n",
      "Epoch 898/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8161 - acc: 0.7925 - val_loss: 0.9388 - val_acc: 0.7490\n",
      "Epoch 899/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8184 - acc: 0.7901 - val_loss: 0.9116 - val_acc: 0.7510\n",
      "Epoch 900/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8192 - acc: 0.7916 - val_loss: 0.9160 - val_acc: 0.7570\n",
      "Epoch 901/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8165 - acc: 0.7893 - val_loss: 0.9432 - val_acc: 0.7360\n",
      "Epoch 902/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8165 - acc: 0.7912 - val_loss: 0.9192 - val_acc: 0.7480\n",
      "Epoch 903/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8175 - acc: 0.7896 - val_loss: 0.9319 - val_acc: 0.7420\n",
      "Epoch 904/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8192 - acc: 0.7899 - val_loss: 0.9496 - val_acc: 0.7540\n",
      "Epoch 905/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8178 - acc: 0.7912 - val_loss: 0.9162 - val_acc: 0.7520\n",
      "Epoch 906/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8146 - acc: 0.7904 - val_loss: 0.9216 - val_acc: 0.7510\n",
      "Epoch 907/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8189 - acc: 0.7891 - val_loss: 0.9598 - val_acc: 0.7460\n",
      "Epoch 908/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8193 - acc: 0.7923 - val_loss: 0.9303 - val_acc: 0.7440\n",
      "Epoch 909/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8182 - acc: 0.7896 - val_loss: 0.9188 - val_acc: 0.7520\n",
      "Epoch 910/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8162 - acc: 0.7903 - val_loss: 0.9227 - val_acc: 0.7520\n",
      "Epoch 911/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8145 - acc: 0.7907 - val_loss: 0.9203 - val_acc: 0.7470\n",
      "Epoch 912/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8152 - acc: 0.7928 - val_loss: 0.9152 - val_acc: 0.7540\n",
      "Epoch 913/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8168 - acc: 0.7900 - val_loss: 0.9222 - val_acc: 0.7460\n",
      "Epoch 914/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8156 - acc: 0.7920 - val_loss: 0.9122 - val_acc: 0.7560\n",
      "Epoch 915/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8154 - acc: 0.7935 - val_loss: 0.9909 - val_acc: 0.7360\n",
      "Epoch 916/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8175 - acc: 0.7887 - val_loss: 0.9163 - val_acc: 0.7580\n",
      "Epoch 917/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8179 - acc: 0.7895 - val_loss: 0.9143 - val_acc: 0.7570\n",
      "Epoch 918/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8136 - acc: 0.7931 - val_loss: 0.9137 - val_acc: 0.7610\n",
      "Epoch 919/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8116 - acc: 0.7940 - val_loss: 0.9339 - val_acc: 0.7530\n",
      "Epoch 920/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8125 - acc: 0.7921 - val_loss: 0.9301 - val_acc: 0.7540\n",
      "Epoch 921/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8117 - acc: 0.7956 - val_loss: 0.9237 - val_acc: 0.7560\n",
      "Epoch 922/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8123 - acc: 0.7952 - val_loss: 0.9273 - val_acc: 0.7510\n",
      "Epoch 923/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8149 - acc: 0.7904 - val_loss: 0.9104 - val_acc: 0.7540\n",
      "Epoch 924/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8121 - acc: 0.7911 - val_loss: 0.9370 - val_acc: 0.7440\n",
      "Epoch 925/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8134 - acc: 0.7920 - val_loss: 0.9335 - val_acc: 0.7480\n",
      "Epoch 926/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8133 - acc: 0.7925 - val_loss: 0.9277 - val_acc: 0.7460\n",
      "Epoch 927/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8141 - acc: 0.7908 - val_loss: 0.9306 - val_acc: 0.7450\n",
      "Epoch 928/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8111 - acc: 0.7921 - val_loss: 0.9263 - val_acc: 0.7520\n",
      "Epoch 929/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8120 - acc: 0.7928 - val_loss: 0.9143 - val_acc: 0.7490\n",
      "Epoch 930/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8127 - acc: 0.7921 - val_loss: 0.9207 - val_acc: 0.7560\n",
      "Epoch 931/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8087 - acc: 0.7921 - val_loss: 0.9123 - val_acc: 0.7460\n",
      "Epoch 932/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8095 - acc: 0.7943 - val_loss: 0.9281 - val_acc: 0.7480\n",
      "Epoch 933/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8126 - acc: 0.7925 - val_loss: 0.9392 - val_acc: 0.7400\n",
      "Epoch 934/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8131 - acc: 0.7905 - val_loss: 0.9296 - val_acc: 0.7470\n",
      "Epoch 935/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8138 - acc: 0.7912 - val_loss: 0.9160 - val_acc: 0.7550\n",
      "Epoch 936/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8130 - acc: 0.7932 - val_loss: 0.9138 - val_acc: 0.7480\n",
      "Epoch 937/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8093 - acc: 0.7964 - val_loss: 0.9302 - val_acc: 0.7540\n",
      "Epoch 938/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8129 - acc: 0.7908 - val_loss: 0.9255 - val_acc: 0.7400\n",
      "Epoch 939/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8097 - acc: 0.7965 - val_loss: 0.9247 - val_acc: 0.7490\n",
      "Epoch 940/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8129 - acc: 0.7921 - val_loss: 0.9541 - val_acc: 0.7420\n",
      "Epoch 941/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8133 - acc: 0.7915 - val_loss: 0.9125 - val_acc: 0.7520\n",
      "Epoch 942/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8095 - acc: 0.7937 - val_loss: 0.9142 - val_acc: 0.7540\n",
      "Epoch 943/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8167 - acc: 0.7951 - val_loss: 0.9315 - val_acc: 0.7550\n",
      "Epoch 944/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8111 - acc: 0.7940 - val_loss: 0.9610 - val_acc: 0.7410\n",
      "Epoch 945/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8145 - acc: 0.7881 - val_loss: 0.9166 - val_acc: 0.7500\n",
      "Epoch 946/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8107 - acc: 0.7944 - val_loss: 0.9243 - val_acc: 0.7590\n",
      "Epoch 947/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8123 - acc: 0.7917 - val_loss: 0.9316 - val_acc: 0.7530\n",
      "Epoch 948/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8111 - acc: 0.7943 - val_loss: 0.9366 - val_acc: 0.7450\n",
      "Epoch 949/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8115 - acc: 0.7937 - val_loss: 0.9270 - val_acc: 0.7540\n",
      "Epoch 950/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8108 - acc: 0.7973 - val_loss: 0.9243 - val_acc: 0.7620\n",
      "Epoch 951/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8123 - acc: 0.7916 - val_loss: 0.9118 - val_acc: 0.7540\n",
      "Epoch 952/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8089 - acc: 0.7935 - val_loss: 0.9118 - val_acc: 0.7520\n",
      "Epoch 953/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8089 - acc: 0.7945 - val_loss: 0.9102 - val_acc: 0.7480\n",
      "Epoch 954/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8153 - acc: 0.7933 - val_loss: 0.9077 - val_acc: 0.7490\n",
      "Epoch 955/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8112 - acc: 0.7960 - val_loss: 0.9812 - val_acc: 0.7410\n",
      "Epoch 956/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8165 - acc: 0.7912 - val_loss: 0.9184 - val_acc: 0.7520\n",
      "Epoch 957/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8123 - acc: 0.7925 - val_loss: 0.9043 - val_acc: 0.7550\n",
      "Epoch 958/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8066 - acc: 0.7964 - val_loss: 0.9124 - val_acc: 0.7540\n",
      "Epoch 959/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8088 - acc: 0.7952 - val_loss: 0.9209 - val_acc: 0.7480\n",
      "Epoch 960/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8102 - acc: 0.7947 - val_loss: 0.9345 - val_acc: 0.7460\n",
      "Epoch 961/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8071 - acc: 0.7947 - val_loss: 0.9116 - val_acc: 0.7490\n",
      "Epoch 962/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8067 - acc: 0.7940 - val_loss: 0.9091 - val_acc: 0.7550\n",
      "Epoch 963/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8098 - acc: 0.7940 - val_loss: 0.9082 - val_acc: 0.7550\n",
      "Epoch 964/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8062 - acc: 0.7967 - val_loss: 0.9157 - val_acc: 0.7430\n",
      "Epoch 965/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8089 - acc: 0.7925 - val_loss: 0.9076 - val_acc: 0.7530\n",
      "Epoch 966/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8133 - acc: 0.7921 - val_loss: 0.9088 - val_acc: 0.7510\n",
      "Epoch 967/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8072 - acc: 0.7948 - val_loss: 0.9273 - val_acc: 0.7450\n",
      "Epoch 968/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8085 - acc: 0.7951 - val_loss: 0.9072 - val_acc: 0.7550\n",
      "Epoch 969/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8070 - acc: 0.7959 - val_loss: 1.0043 - val_acc: 0.7340\n",
      "Epoch 970/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8103 - acc: 0.7929 - val_loss: 0.9843 - val_acc: 0.7430\n",
      "Epoch 971/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8095 - acc: 0.7956 - val_loss: 0.9279 - val_acc: 0.7470\n",
      "Epoch 972/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8063 - acc: 0.7933 - val_loss: 0.9093 - val_acc: 0.7510\n",
      "Epoch 973/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8082 - acc: 0.7944 - val_loss: 0.9271 - val_acc: 0.7540\n",
      "Epoch 974/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8092 - acc: 0.7943 - val_loss: 0.9094 - val_acc: 0.7480\n",
      "Epoch 975/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8064 - acc: 0.7972 - val_loss: 0.9291 - val_acc: 0.7590\n",
      "Epoch 976/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8079 - acc: 0.7947 - val_loss: 0.9253 - val_acc: 0.7530\n",
      "Epoch 977/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8072 - acc: 0.7948 - val_loss: 0.9154 - val_acc: 0.7500\n",
      "Epoch 978/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8103 - acc: 0.7953 - val_loss: 0.9131 - val_acc: 0.7450\n",
      "Epoch 979/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8094 - acc: 0.7943 - val_loss: 0.9105 - val_acc: 0.7570\n",
      "Epoch 980/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8064 - acc: 0.7949 - val_loss: 0.9167 - val_acc: 0.7540\n",
      "Epoch 981/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8100 - acc: 0.7933 - val_loss: 0.9316 - val_acc: 0.7470\n",
      "Epoch 982/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8061 - acc: 0.7972 - val_loss: 0.9114 - val_acc: 0.7550\n",
      "Epoch 983/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8073 - acc: 0.7955 - val_loss: 0.9264 - val_acc: 0.7520\n",
      "Epoch 984/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8051 - acc: 0.7960 - val_loss: 0.9106 - val_acc: 0.7500\n",
      "Epoch 985/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8079 - acc: 0.7965 - val_loss: 0.9175 - val_acc: 0.7540\n",
      "Epoch 986/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8037 - acc: 0.7975 - val_loss: 0.9091 - val_acc: 0.7540\n",
      "Epoch 987/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8081 - acc: 0.7948 - val_loss: 0.9133 - val_acc: 0.7560\n",
      "Epoch 988/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8069 - acc: 0.7971 - val_loss: 0.9199 - val_acc: 0.7570\n",
      "Epoch 989/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8076 - acc: 0.7953 - val_loss: 0.9250 - val_acc: 0.7540\n",
      "Epoch 990/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8072 - acc: 0.7951 - val_loss: 0.9121 - val_acc: 0.7530\n",
      "Epoch 991/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8055 - acc: 0.7945 - val_loss: 0.9193 - val_acc: 0.7440\n",
      "Epoch 992/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8068 - acc: 0.7940 - val_loss: 0.9741 - val_acc: 0.7440\n",
      "Epoch 993/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8053 - acc: 0.7971 - val_loss: 0.9071 - val_acc: 0.7550\n",
      "Epoch 994/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8050 - acc: 0.7963 - val_loss: 0.9794 - val_acc: 0.7410\n",
      "Epoch 995/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8151 - acc: 0.7943 - val_loss: 0.9622 - val_acc: 0.7430\n",
      "Epoch 996/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8088 - acc: 0.7937 - val_loss: 0.9086 - val_acc: 0.7460\n",
      "Epoch 997/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8085 - acc: 0.7935 - val_loss: 0.9557 - val_acc: 0.7550\n",
      "Epoch 998/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8043 - acc: 0.7977 - val_loss: 0.9087 - val_acc: 0.7600\n",
      "Epoch 999/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8029 - acc: 0.7947 - val_loss: 0.9669 - val_acc: 0.7510\n",
      "Epoch 1000/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8047 - acc: 0.7981 - val_loss: 0.9262 - val_acc: 0.7490\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step\n",
      "1500/1500 [==============================] - 0s 32us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.817813452021281, 0.794]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9234442513783773, 0.7406666668256124]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 1s 112us/step - loss: 1.9913 - acc: 0.1449 - val_loss: 1.9466 - val_acc: 0.1590\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.9636 - acc: 0.1597 - val_loss: 1.9323 - val_acc: 0.1740\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 1.9497 - acc: 0.1709 - val_loss: 1.9212 - val_acc: 0.2040\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.9411 - acc: 0.1784 - val_loss: 1.9131 - val_acc: 0.2140\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.9301 - acc: 0.1903 - val_loss: 1.9038 - val_acc: 0.2310\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.9194 - acc: 0.1939 - val_loss: 1.8945 - val_acc: 0.2330\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.9130 - acc: 0.2003 - val_loss: 1.8854 - val_acc: 0.2410\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.9043 - acc: 0.2097 - val_loss: 1.8754 - val_acc: 0.2390\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.8949 - acc: 0.2148 - val_loss: 1.8642 - val_acc: 0.2470\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.8830 - acc: 0.2236 - val_loss: 1.8502 - val_acc: 0.2490\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.8739 - acc: 0.2255 - val_loss: 1.8356 - val_acc: 0.2610\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.8653 - acc: 0.2368 - val_loss: 1.8209 - val_acc: 0.2770\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.8512 - acc: 0.2491 - val_loss: 1.8039 - val_acc: 0.2850\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.8318 - acc: 0.2628 - val_loss: 1.7833 - val_acc: 0.3080\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.8208 - acc: 0.2657 - val_loss: 1.7607 - val_acc: 0.3350\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.8116 - acc: 0.2715 - val_loss: 1.7410 - val_acc: 0.3750\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.7866 - acc: 0.2861 - val_loss: 1.7154 - val_acc: 0.4080\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.7702 - acc: 0.2988 - val_loss: 1.6905 - val_acc: 0.4250\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.7439 - acc: 0.3048 - val_loss: 1.6607 - val_acc: 0.4340\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.7331 - acc: 0.3211 - val_loss: 1.6327 - val_acc: 0.4550\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.7103 - acc: 0.3283 - val_loss: 1.6049 - val_acc: 0.4720\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.6966 - acc: 0.3313 - val_loss: 1.5772 - val_acc: 0.4990\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.6655 - acc: 0.3515 - val_loss: 1.5468 - val_acc: 0.5040\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.6491 - acc: 0.3555 - val_loss: 1.5166 - val_acc: 0.5170\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.6275 - acc: 0.3684 - val_loss: 1.4886 - val_acc: 0.5330\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.6082 - acc: 0.3644 - val_loss: 1.4617 - val_acc: 0.5540\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.5867 - acc: 0.3852 - val_loss: 1.4346 - val_acc: 0.5520\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.5681 - acc: 0.3804 - val_loss: 1.4092 - val_acc: 0.5750\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.5492 - acc: 0.3977 - val_loss: 1.3844 - val_acc: 0.5910\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.5344 - acc: 0.4065 - val_loss: 1.3590 - val_acc: 0.5950\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.5096 - acc: 0.4175 - val_loss: 1.3322 - val_acc: 0.6120\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.5006 - acc: 0.4197 - val_loss: 1.3125 - val_acc: 0.6190\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.4783 - acc: 0.4220 - val_loss: 1.2917 - val_acc: 0.6390\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.4627 - acc: 0.4388 - val_loss: 1.2686 - val_acc: 0.6410\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.4398 - acc: 0.4496 - val_loss: 1.2466 - val_acc: 0.6580\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.4298 - acc: 0.4457 - val_loss: 1.2292 - val_acc: 0.6500\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.4177 - acc: 0.4580 - val_loss: 1.2120 - val_acc: 0.6720\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.3965 - acc: 0.4732 - val_loss: 1.1890 - val_acc: 0.6690\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.3913 - acc: 0.4679 - val_loss: 1.1727 - val_acc: 0.6740\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.3679 - acc: 0.4848 - val_loss: 1.1534 - val_acc: 0.6800\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.3506 - acc: 0.4936 - val_loss: 1.1374 - val_acc: 0.6790\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.3310 - acc: 0.4964 - val_loss: 1.1188 - val_acc: 0.6900\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.3327 - acc: 0.4905 - val_loss: 1.1010 - val_acc: 0.6940\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.3171 - acc: 0.5084 - val_loss: 1.0860 - val_acc: 0.6970\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3004 - acc: 0.5064 - val_loss: 1.0692 - val_acc: 0.7050\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2875 - acc: 0.5201 - val_loss: 1.0537 - val_acc: 0.7100\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.2814 - acc: 0.5176 - val_loss: 1.0439 - val_acc: 0.7140\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.2663 - acc: 0.5285 - val_loss: 1.0285 - val_acc: 0.7170\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.2506 - acc: 0.5304 - val_loss: 1.0132 - val_acc: 0.7210\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.2487 - acc: 0.5323 - val_loss: 1.0042 - val_acc: 0.7200\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.2447 - acc: 0.5351 - val_loss: 0.9920 - val_acc: 0.7250\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.2188 - acc: 0.5421 - val_loss: 0.9798 - val_acc: 0.7220\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.2028 - acc: 0.5515 - val_loss: 0.9684 - val_acc: 0.7250\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.2076 - acc: 0.5508 - val_loss: 0.9567 - val_acc: 0.7270\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.2097 - acc: 0.5425 - val_loss: 0.9521 - val_acc: 0.7330\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1932 - acc: 0.5524 - val_loss: 0.9419 - val_acc: 0.7340\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.1681 - acc: 0.5607 - val_loss: 0.9258 - val_acc: 0.7320\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.1527 - acc: 0.5728 - val_loss: 0.9139 - val_acc: 0.7320\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.1519 - acc: 0.5725 - val_loss: 0.9067 - val_acc: 0.7360\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.1522 - acc: 0.5735 - val_loss: 0.8988 - val_acc: 0.7320\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.1519 - acc: 0.5700 - val_loss: 0.8927 - val_acc: 0.7330\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.1345 - acc: 0.5781 - val_loss: 0.8846 - val_acc: 0.7330\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.1257 - acc: 0.5879 - val_loss: 0.8769 - val_acc: 0.7390\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.1222 - acc: 0.5861 - val_loss: 0.8696 - val_acc: 0.7400\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.1106 - acc: 0.5889 - val_loss: 0.8570 - val_acc: 0.7410\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1143 - acc: 0.5865 - val_loss: 0.8521 - val_acc: 0.7400\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.0894 - acc: 0.5897 - val_loss: 0.8479 - val_acc: 0.7390\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0799 - acc: 0.6011 - val_loss: 0.8399 - val_acc: 0.7390\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0843 - acc: 0.5935 - val_loss: 0.8339 - val_acc: 0.7440\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0802 - acc: 0.6041 - val_loss: 0.8267 - val_acc: 0.7430\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0776 - acc: 0.5957 - val_loss: 0.8227 - val_acc: 0.7440\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0476 - acc: 0.6100 - val_loss: 0.8140 - val_acc: 0.7410\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0552 - acc: 0.6115 - val_loss: 0.8102 - val_acc: 0.7420\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0450 - acc: 0.6141 - val_loss: 0.8031 - val_acc: 0.7430\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0421 - acc: 0.6175 - val_loss: 0.7982 - val_acc: 0.7440\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0304 - acc: 0.6172 - val_loss: 0.7922 - val_acc: 0.7480\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0384 - acc: 0.6168 - val_loss: 0.7895 - val_acc: 0.7460\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0387 - acc: 0.6169 - val_loss: 0.7843 - val_acc: 0.7490\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0345 - acc: 0.6125 - val_loss: 0.7840 - val_acc: 0.7490\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0303 - acc: 0.6168 - val_loss: 0.7812 - val_acc: 0.7480\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0076 - acc: 0.6255 - val_loss: 0.7724 - val_acc: 0.7470\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.0039 - acc: 0.6308 - val_loss: 0.7680 - val_acc: 0.7540\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9973 - acc: 0.6347 - val_loss: 0.7606 - val_acc: 0.7510\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9790 - acc: 0.6373 - val_loss: 0.7595 - val_acc: 0.7560\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9830 - acc: 0.6376 - val_loss: 0.7538 - val_acc: 0.7580\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9808 - acc: 0.6384 - val_loss: 0.7505 - val_acc: 0.7580\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9841 - acc: 0.6319 - val_loss: 0.7472 - val_acc: 0.7550\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9880 - acc: 0.6313 - val_loss: 0.7465 - val_acc: 0.7560\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9740 - acc: 0.6427 - val_loss: 0.7418 - val_acc: 0.7570\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9644 - acc: 0.6392 - val_loss: 0.7356 - val_acc: 0.7610\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9573 - acc: 0.6437 - val_loss: 0.7328 - val_acc: 0.7590\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9568 - acc: 0.6459 - val_loss: 0.7304 - val_acc: 0.7550\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9580 - acc: 0.6436 - val_loss: 0.7286 - val_acc: 0.7600\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9566 - acc: 0.6472 - val_loss: 0.7247 - val_acc: 0.7580\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9477 - acc: 0.6548 - val_loss: 0.7213 - val_acc: 0.7590\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9478 - acc: 0.6485 - val_loss: 0.7196 - val_acc: 0.7590\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9443 - acc: 0.6496 - val_loss: 0.7173 - val_acc: 0.7600\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9405 - acc: 0.6437 - val_loss: 0.7152 - val_acc: 0.7610\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9374 - acc: 0.6535 - val_loss: 0.7151 - val_acc: 0.7640\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9261 - acc: 0.6527 - val_loss: 0.7104 - val_acc: 0.7660\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9297 - acc: 0.6520 - val_loss: 0.7094 - val_acc: 0.7650\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9185 - acc: 0.6611 - val_loss: 0.7039 - val_acc: 0.7670\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9234 - acc: 0.6632 - val_loss: 0.7018 - val_acc: 0.7630\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9134 - acc: 0.6597 - val_loss: 0.6990 - val_acc: 0.7650\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9139 - acc: 0.6549 - val_loss: 0.6966 - val_acc: 0.7670\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9137 - acc: 0.6643 - val_loss: 0.6974 - val_acc: 0.7660\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.9002 - acc: 0.6625 - val_loss: 0.6923 - val_acc: 0.7670\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8999 - acc: 0.6675 - val_loss: 0.6890 - val_acc: 0.7650\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9050 - acc: 0.6637 - val_loss: 0.6897 - val_acc: 0.7680\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9015 - acc: 0.6572 - val_loss: 0.6882 - val_acc: 0.7680\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8923 - acc: 0.6672 - val_loss: 0.6841 - val_acc: 0.7700\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8915 - acc: 0.6696 - val_loss: 0.6817 - val_acc: 0.7670\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8944 - acc: 0.6672 - val_loss: 0.6806 - val_acc: 0.7660\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8940 - acc: 0.6643 - val_loss: 0.6791 - val_acc: 0.7720\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8823 - acc: 0.6693 - val_loss: 0.6774 - val_acc: 0.7700\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8847 - acc: 0.6720 - val_loss: 0.6765 - val_acc: 0.7720\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8787 - acc: 0.6700 - val_loss: 0.6739 - val_acc: 0.7670\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8736 - acc: 0.6668 - val_loss: 0.6751 - val_acc: 0.7690\n",
      "Epoch 119/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8818 - acc: 0.6720 - val_loss: 0.6739 - val_acc: 0.7690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8839 - acc: 0.6717 - val_loss: 0.6714 - val_acc: 0.7690\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8632 - acc: 0.6775 - val_loss: 0.6691 - val_acc: 0.7710\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8740 - acc: 0.6729 - val_loss: 0.6693 - val_acc: 0.7720\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8594 - acc: 0.6755 - val_loss: 0.6665 - val_acc: 0.7730\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8618 - acc: 0.6777 - val_loss: 0.6648 - val_acc: 0.7730\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8580 - acc: 0.6851 - val_loss: 0.6623 - val_acc: 0.7690\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8598 - acc: 0.6753 - val_loss: 0.6619 - val_acc: 0.7710\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8557 - acc: 0.6847 - val_loss: 0.6599 - val_acc: 0.7720\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8531 - acc: 0.6723 - val_loss: 0.6595 - val_acc: 0.7710\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8431 - acc: 0.6823 - val_loss: 0.6592 - val_acc: 0.7670\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8467 - acc: 0.6875 - val_loss: 0.6559 - val_acc: 0.7750\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8386 - acc: 0.6908 - val_loss: 0.6552 - val_acc: 0.7720\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8493 - acc: 0.6787 - val_loss: 0.6541 - val_acc: 0.7750\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8512 - acc: 0.6791 - val_loss: 0.6561 - val_acc: 0.7720\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8324 - acc: 0.6896 - val_loss: 0.6534 - val_acc: 0.7750\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8393 - acc: 0.6869 - val_loss: 0.6522 - val_acc: 0.7730\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8306 - acc: 0.6951 - val_loss: 0.6500 - val_acc: 0.7740\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8173 - acc: 0.6913 - val_loss: 0.6485 - val_acc: 0.7710\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8219 - acc: 0.6900 - val_loss: 0.6474 - val_acc: 0.7730\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8167 - acc: 0.6964 - val_loss: 0.6449 - val_acc: 0.7720\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8158 - acc: 0.6932 - val_loss: 0.6455 - val_acc: 0.7710\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8181 - acc: 0.6947 - val_loss: 0.6442 - val_acc: 0.7730\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8117 - acc: 0.6955 - val_loss: 0.6451 - val_acc: 0.7740\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8150 - acc: 0.6888 - val_loss: 0.6428 - val_acc: 0.7700\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8131 - acc: 0.6965 - val_loss: 0.6421 - val_acc: 0.7720\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8090 - acc: 0.6961 - val_loss: 0.6420 - val_acc: 0.7700\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8128 - acc: 0.6973 - val_loss: 0.6403 - val_acc: 0.7700\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8095 - acc: 0.6995 - val_loss: 0.6415 - val_acc: 0.7720\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8145 - acc: 0.6980 - val_loss: 0.6390 - val_acc: 0.7730\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8007 - acc: 0.6989 - val_loss: 0.6390 - val_acc: 0.7720\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.7864 - acc: 0.7043 - val_loss: 0.6362 - val_acc: 0.7730\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.7872 - acc: 0.7012 - val_loss: 0.6355 - val_acc: 0.7720\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8028 - acc: 0.7011 - val_loss: 0.6361 - val_acc: 0.7740\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.7867 - acc: 0.7081 - val_loss: 0.6340 - val_acc: 0.7700\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.7996 - acc: 0.6992 - val_loss: 0.6347 - val_acc: 0.7740\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8031 - acc: 0.6972 - val_loss: 0.6366 - val_acc: 0.7760\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8032 - acc: 0.6969 - val_loss: 0.6355 - val_acc: 0.7730\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.7902 - acc: 0.7047 - val_loss: 0.6329 - val_acc: 0.7710\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.7845 - acc: 0.7121 - val_loss: 0.6322 - val_acc: 0.7730\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.7813 - acc: 0.7087 - val_loss: 0.6304 - val_acc: 0.7730\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.7854 - acc: 0.7057 - val_loss: 0.6302 - val_acc: 0.7720\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.7819 - acc: 0.7004 - val_loss: 0.6287 - val_acc: 0.7750\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.7809 - acc: 0.7092 - val_loss: 0.6301 - val_acc: 0.7780\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.7804 - acc: 0.7069 - val_loss: 0.6278 - val_acc: 0.7760\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.7731 - acc: 0.7127 - val_loss: 0.6281 - val_acc: 0.7780\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.7807 - acc: 0.7025 - val_loss: 0.6280 - val_acc: 0.7760\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.7685 - acc: 0.7140 - val_loss: 0.6271 - val_acc: 0.7760\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.7731 - acc: 0.7077 - val_loss: 0.6276 - val_acc: 0.7730\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.7745 - acc: 0.7136 - val_loss: 0.6302 - val_acc: 0.7730\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.7665 - acc: 0.7119 - val_loss: 0.6247 - val_acc: 0.7710\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.7635 - acc: 0.7136 - val_loss: 0.6245 - val_acc: 0.7750\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.7606 - acc: 0.7144 - val_loss: 0.6242 - val_acc: 0.7770\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.7654 - acc: 0.7104 - val_loss: 0.6247 - val_acc: 0.7770\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.7761 - acc: 0.7072 - val_loss: 0.6242 - val_acc: 0.7790\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.7600 - acc: 0.7203 - val_loss: 0.6231 - val_acc: 0.7750\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.7696 - acc: 0.7043 - val_loss: 0.6240 - val_acc: 0.7770\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.7687 - acc: 0.7103 - val_loss: 0.6230 - val_acc: 0.7780\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.7645 - acc: 0.7108 - val_loss: 0.6213 - val_acc: 0.7780\n",
      "Epoch 178/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.7446 - acc: 0.7157 - val_loss: 0.6217 - val_acc: 0.7780\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.7469 - acc: 0.7167 - val_loss: 0.6198 - val_acc: 0.7770\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.7387 - acc: 0.7168 - val_loss: 0.6190 - val_acc: 0.7800\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.7476 - acc: 0.7179 - val_loss: 0.6184 - val_acc: 0.7830\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.7524 - acc: 0.7223 - val_loss: 0.6189 - val_acc: 0.7790\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.7468 - acc: 0.7205 - val_loss: 0.6182 - val_acc: 0.7750\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.7375 - acc: 0.7177 - val_loss: 0.6193 - val_acc: 0.7790\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.7594 - acc: 0.7167 - val_loss: 0.6192 - val_acc: 0.7760\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.7433 - acc: 0.7239 - val_loss: 0.6199 - val_acc: 0.7790\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.7327 - acc: 0.7227 - val_loss: 0.6178 - val_acc: 0.7780\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.7413 - acc: 0.7193 - val_loss: 0.6168 - val_acc: 0.7790\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.7341 - acc: 0.7261 - val_loss: 0.6153 - val_acc: 0.7790\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.7499 - acc: 0.7200 - val_loss: 0.6167 - val_acc: 0.7770\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.7544 - acc: 0.7131 - val_loss: 0.6159 - val_acc: 0.7840\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7389 - acc: 0.7209 - val_loss: 0.6127 - val_acc: 0.7820\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7396 - acc: 0.7223 - val_loss: 0.6151 - val_acc: 0.7840\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.7281 - acc: 0.7220 - val_loss: 0.6147 - val_acc: 0.7790\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.7360 - acc: 0.7216 - val_loss: 0.6114 - val_acc: 0.7830\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.7395 - acc: 0.7213 - val_loss: 0.6137 - val_acc: 0.7810\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.7294 - acc: 0.7253 - val_loss: 0.6136 - val_acc: 0.7790\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.7396 - acc: 0.7189 - val_loss: 0.6123 - val_acc: 0.7810\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.7317 - acc: 0.7245 - val_loss: 0.6127 - val_acc: 0.7810\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7133 - acc: 0.7304 - val_loss: 0.6140 - val_acc: 0.7750\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 24us/step\n",
      "1500/1500 [==============================] - 0s 26us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.44953240927060445, 0.8355999999682109]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6567809325853984, 0.745333333492279]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 1.9131 - acc: 0.1977 - val_loss: 1.8734 - val_acc: 0.2517\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 1.8204 - acc: 0.3034 - val_loss: 1.7551 - val_acc: 0.3397\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 1.6686 - acc: 0.4072 - val_loss: 1.5741 - val_acc: 0.4647\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 1.4662 - acc: 0.5248 - val_loss: 1.3619 - val_acc: 0.5560\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 1.2557 - acc: 0.6060 - val_loss: 1.1666 - val_acc: 0.6303\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 1.0768 - acc: 0.6660 - val_loss: 1.0120 - val_acc: 0.6777\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.9451 - acc: 0.7012 - val_loss: 0.9037 - val_acc: 0.7047\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.8536 - acc: 0.7191 - val_loss: 0.8281 - val_acc: 0.7210\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.7894 - acc: 0.7321 - val_loss: 0.7750 - val_acc: 0.7300\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.7434 - acc: 0.7419 - val_loss: 0.7363 - val_acc: 0.7397\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.7086 - acc: 0.7492 - val_loss: 0.7075 - val_acc: 0.7450\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6815 - acc: 0.7555 - val_loss: 0.6846 - val_acc: 0.7507\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6596 - acc: 0.7612 - val_loss: 0.6661 - val_acc: 0.7587\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6413 - acc: 0.7678 - val_loss: 0.6501 - val_acc: 0.7623\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.6259 - acc: 0.7724 - val_loss: 0.6383 - val_acc: 0.7653\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.6119 - acc: 0.7780 - val_loss: 0.6275 - val_acc: 0.7700\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5999 - acc: 0.7809 - val_loss: 0.6168 - val_acc: 0.7720\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5891 - acc: 0.7845 - val_loss: 0.6079 - val_acc: 0.7757\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5792 - acc: 0.7892 - val_loss: 0.6013 - val_acc: 0.7753\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5700 - acc: 0.7920 - val_loss: 0.5924 - val_acc: 0.7813\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5614 - acc: 0.7954 - val_loss: 0.5879 - val_acc: 0.7807\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5535 - acc: 0.7989 - val_loss: 0.5832 - val_acc: 0.7817\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5459 - acc: 0.8010 - val_loss: 0.5766 - val_acc: 0.7847\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5391 - acc: 0.8044 - val_loss: 0.5735 - val_acc: 0.7850\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5326 - acc: 0.8076 - val_loss: 0.5674 - val_acc: 0.7937\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5264 - acc: 0.8092 - val_loss: 0.5622 - val_acc: 0.7920\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5198 - acc: 0.8114 - val_loss: 0.5599 - val_acc: 0.7977\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5144 - acc: 0.8140 - val_loss: 0.5571 - val_acc: 0.8000\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5087 - acc: 0.8162 - val_loss: 0.5509 - val_acc: 0.8000\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5033 - acc: 0.8180 - val_loss: 0.5483 - val_acc: 0.8020\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4982 - acc: 0.8205 - val_loss: 0.5443 - val_acc: 0.8023\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4934 - acc: 0.8222 - val_loss: 0.5435 - val_acc: 0.8027\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4885 - acc: 0.8253 - val_loss: 0.5426 - val_acc: 0.8033\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4840 - acc: 0.8256 - val_loss: 0.5386 - val_acc: 0.8080\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4799 - acc: 0.8278 - val_loss: 0.5341 - val_acc: 0.8093\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4755 - acc: 0.8305 - val_loss: 0.5322 - val_acc: 0.8100\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4713 - acc: 0.8308 - val_loss: 0.5297 - val_acc: 0.8117\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4674 - acc: 0.8319 - val_loss: 0.5273 - val_acc: 0.8123\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4632 - acc: 0.8339 - val_loss: 0.5265 - val_acc: 0.8103\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4599 - acc: 0.8355 - val_loss: 0.5236 - val_acc: 0.8103\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4558 - acc: 0.8370 - val_loss: 0.5241 - val_acc: 0.8103\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4522 - acc: 0.8383 - val_loss: 0.5210 - val_acc: 0.8120\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4487 - acc: 0.8403 - val_loss: 0.5223 - val_acc: 0.8143\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4453 - acc: 0.8405 - val_loss: 0.5187 - val_acc: 0.8180\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4420 - acc: 0.8427 - val_loss: 0.5206 - val_acc: 0.8153\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4388 - acc: 0.8437 - val_loss: 0.5186 - val_acc: 0.8120\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4358 - acc: 0.8442 - val_loss: 0.5154 - val_acc: 0.8133\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4324 - acc: 0.8458 - val_loss: 0.5156 - val_acc: 0.8130\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4296 - acc: 0.8474 - val_loss: 0.5147 - val_acc: 0.8150\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4266 - acc: 0.8484 - val_loss: 0.5136 - val_acc: 0.8117\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4235 - acc: 0.8492 - val_loss: 0.5142 - val_acc: 0.8167\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4206 - acc: 0.8502 - val_loss: 0.5135 - val_acc: 0.8133\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4181 - acc: 0.8508 - val_loss: 0.5154 - val_acc: 0.8163\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4152 - acc: 0.8521 - val_loss: 0.5109 - val_acc: 0.8140\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4125 - acc: 0.8531 - val_loss: 0.5124 - val_acc: 0.8160\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4100 - acc: 0.8537 - val_loss: 0.5126 - val_acc: 0.8163\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4076 - acc: 0.8543 - val_loss: 0.5112 - val_acc: 0.8180\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4052 - acc: 0.8555 - val_loss: 0.5120 - val_acc: 0.8113\n",
      "Epoch 59/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4028 - acc: 0.8556 - val_loss: 0.5111 - val_acc: 0.8130\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4000 - acc: 0.8580 - val_loss: 0.5105 - val_acc: 0.8183\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3977 - acc: 0.8590 - val_loss: 0.5116 - val_acc: 0.8163\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3953 - acc: 0.8602 - val_loss: 0.5123 - val_acc: 0.8180\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3931 - acc: 0.8606 - val_loss: 0.5089 - val_acc: 0.8157\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3908 - acc: 0.8617 - val_loss: 0.5120 - val_acc: 0.8113\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3888 - acc: 0.8624 - val_loss: 0.5128 - val_acc: 0.8157\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3863 - acc: 0.8633 - val_loss: 0.5122 - val_acc: 0.8167\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3844 - acc: 0.8638 - val_loss: 0.5100 - val_acc: 0.8157\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3821 - acc: 0.8646 - val_loss: 0.5113 - val_acc: 0.8160\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3801 - acc: 0.8665 - val_loss: 0.5136 - val_acc: 0.8120\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3779 - acc: 0.8661 - val_loss: 0.5121 - val_acc: 0.8163\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3760 - acc: 0.8688 - val_loss: 0.5113 - val_acc: 0.8117\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3741 - acc: 0.8685 - val_loss: 0.5115 - val_acc: 0.8163\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3720 - acc: 0.8689 - val_loss: 0.5121 - val_acc: 0.8167\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3702 - acc: 0.8699 - val_loss: 0.5157 - val_acc: 0.8160\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3681 - acc: 0.8702 - val_loss: 0.5137 - val_acc: 0.8160\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3661 - acc: 0.8725 - val_loss: 0.5126 - val_acc: 0.8143\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3641 - acc: 0.8718 - val_loss: 0.5138 - val_acc: 0.8147\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3627 - acc: 0.8725 - val_loss: 0.5194 - val_acc: 0.8160\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3610 - acc: 0.8739 - val_loss: 0.5152 - val_acc: 0.8117\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3589 - acc: 0.8742 - val_loss: 0.5166 - val_acc: 0.8170\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3571 - acc: 0.8754 - val_loss: 0.5157 - val_acc: 0.8147\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3557 - acc: 0.8765 - val_loss: 0.5159 - val_acc: 0.8150\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3536 - acc: 0.8771 - val_loss: 0.5180 - val_acc: 0.8157\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3520 - acc: 0.8768 - val_loss: 0.5189 - val_acc: 0.8140\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3501 - acc: 0.8779 - val_loss: 0.5177 - val_acc: 0.8160\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3487 - acc: 0.8785 - val_loss: 0.5218 - val_acc: 0.8167\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3468 - acc: 0.8794 - val_loss: 0.5212 - val_acc: 0.8137\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3456 - acc: 0.8793 - val_loss: 0.5198 - val_acc: 0.8153\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3438 - acc: 0.8801 - val_loss: 0.5210 - val_acc: 0.8143\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3421 - acc: 0.8802 - val_loss: 0.5235 - val_acc: 0.8127\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3406 - acc: 0.8813 - val_loss: 0.5213 - val_acc: 0.8143\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3386 - acc: 0.8818 - val_loss: 0.5223 - val_acc: 0.8153\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3374 - acc: 0.8827 - val_loss: 0.5232 - val_acc: 0.8137\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3358 - acc: 0.8840 - val_loss: 0.5240 - val_acc: 0.8150\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3342 - acc: 0.8832 - val_loss: 0.5284 - val_acc: 0.8160\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3328 - acc: 0.8852 - val_loss: 0.5263 - val_acc: 0.8160\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3312 - acc: 0.8856 - val_loss: 0.5260 - val_acc: 0.8137\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3297 - acc: 0.8869 - val_loss: 0.5322 - val_acc: 0.8117\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3280 - acc: 0.8863 - val_loss: 0.5297 - val_acc: 0.8140\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3267 - acc: 0.8873 - val_loss: 0.5302 - val_acc: 0.8127\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3257 - acc: 0.8878 - val_loss: 0.5295 - val_acc: 0.8133\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3239 - acc: 0.8889 - val_loss: 0.5335 - val_acc: 0.8143\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3223 - acc: 0.8888 - val_loss: 0.5320 - val_acc: 0.8153\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3212 - acc: 0.8890 - val_loss: 0.5335 - val_acc: 0.8130\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3196 - acc: 0.8891 - val_loss: 0.5339 - val_acc: 0.8150\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3184 - acc: 0.8903 - val_loss: 0.5370 - val_acc: 0.8143\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3171 - acc: 0.8912 - val_loss: 0.5352 - val_acc: 0.8147\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3152 - acc: 0.8909 - val_loss: 0.5379 - val_acc: 0.8127\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3144 - acc: 0.8923 - val_loss: 0.5363 - val_acc: 0.8137\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3130 - acc: 0.8931 - val_loss: 0.5379 - val_acc: 0.8133\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3114 - acc: 0.8927 - val_loss: 0.5388 - val_acc: 0.8153\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3100 - acc: 0.8924 - val_loss: 0.5392 - val_acc: 0.8147\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3091 - acc: 0.8947 - val_loss: 0.5406 - val_acc: 0.8137\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3078 - acc: 0.8943 - val_loss: 0.5422 - val_acc: 0.8157\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3061 - acc: 0.8951 - val_loss: 0.5433 - val_acc: 0.8123\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3052 - acc: 0.8956 - val_loss: 0.5432 - val_acc: 0.8130\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3035 - acc: 0.8950 - val_loss: 0.5483 - val_acc: 0.8090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3024 - acc: 0.8965 - val_loss: 0.5461 - val_acc: 0.8117\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3013 - acc: 0.8966 - val_loss: 0.5459 - val_acc: 0.8127\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.2998 - acc: 0.8972 - val_loss: 0.5460 - val_acc: 0.8150\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 29us/step\n",
      "1500/1500 [==============================] - 0s 28us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4486694651126862, 0.8275999999682109]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.587185098807017, 0.7699999995231629]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
